[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wk1.html",
    "href": "wk1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This week we were introduced to remote sensing, focusing on how satellite imagery is collected. Firstly, there are two types of sources, passive and active - which use their own energy source to monitor Earth (e.g. SAR). The signal is recorded by capturing electromagnetic waves, which can have different frequencies, enabling sensors to find collect information about things that are not visible to human eye, because different objects have different spectral signature."
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "2  Week 2 - Xaringan Presentation",
    "section": "",
    "text": "Here is the presentation on Landsat 9:"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "::: {#refs}\n\n\n\n\n\nLaw, W Boone, Peter Hiscock, Bertram Ostendorf, and Megan Lewis. 2021.\n“Using Satellite Imagery to Evaluate Precontact Aboriginal\nForaging Habitats in the Australian Western Desert.”\nScientific Reports 11 (1): 10755."
  },
  {
    "objectID": "index.html#casa0023-learning-diary",
    "href": "index.html#casa0023-learning-diary",
    "title": "CASA0023",
    "section": "CASA0023 Learning Diary",
    "text": "CASA0023 Learning Diary\nThis is the Portfolio for CASA0023 Remotely Sensing Cities and Environments, created with Quarto."
  },
  {
    "objectID": "wk1.html#introduction-to-remote-sensing",
    "href": "wk1.html#introduction-to-remote-sensing",
    "title": "1  Week 1",
    "section": "1.1 Introduction to Remote Sensing",
    "text": "1.1 Introduction to Remote Sensing\nIn the first week of the module we were introduced to remote sensing, focusing on the principles of what satellite imagery is and how it’s collected. The first classification of sensor types, is into passive and active sources, the former use the sun’s energy reflected off the ground, while the latter use their own energy source to send signal to Earth and record it (e.g. SAR). The sensors record the signal by capturing electromagnetic waves, which can have different frequencies, enabling sensors to collect information about things that are not visible to human eye, due to varying spectral signatures of different objects. The figure below shows such a spectrum of wavelengths that signal can take, showing a rather small part that is actually visible.\n\n\n\nElectromagnetic spectrum. Image source: SkyWatch.com\n\n\nThere are certain important considerations that we need to be aware of about the signal that is being collected by the satellites. Firstly, as the signal travels, it passes through the atmosphere, which can interrupt it causing atmospheric haze of various types. Furthermore, because smaller wavelengths scatter easier, they are more difficult to pick up by the sensor.\n\n1.1.1 The Four Resolutions\nThere are four resolutions of images captured by satellites. Firstly, spatial - which tells us how spatially precise an image is, e.g. 10m per px. Spectral resolution refers to the bands which have been recorded, as not all wavelengths are observable; images with many bands are referred to as multispectral or hyperspectral. Radiometric resolution describes the sensitivity e.g. 8-bit; and finally the temporal resolution is how often the certain place on Earth is photographed.\n\n\n1.1.2 Merging bands\nIndividual bands can be merged to create indices, which show specific characteristics of the Earth’s surface we want to measure. In a situation where bands are not in the same spatial resolutions, images need to be resampled into the same resolution, in order to be able to work with such data conjointly. There are two options for this: downscaling - going from a higher resolution like 10m to a lower like 30m; or upscaling, which is the opposite operation. The former is preferred in most cases, as the transformation makes fewer assumptions. An example of an index that can be computed by merging specific bands in a certain way is the tasseled cap transformation, which is calculated from the bands representing brightness, greenness and wetness. The resulting index can be used to identify vegetation, as well as urban areas.\n\n\n1.1.3 Working with sattelite imagery data\nSatellite data taken by Sentinel satellites can be freely accessed from the Copernicus Open Access Hub. Software tools like QGIS and SNAP (specifically for Sentinel data) can be used to browse and analyse this data, e.g. for combining different bands to produce desired indices. Examples of such indices include false colour composite, or atmospheric penetration composite, which allow us to visualise features of the environment that are not visible to the human eye. Landsat images, which are free global images of temporal resolution 16 days, can also be acquired from the USGS website."
  },
  {
    "objectID": "wk1.html#practcial-examples",
    "href": "wk1.html#practcial-examples",
    "title": "1  Week 1",
    "section": "1.2 Practcial examples",
    "text": "1.2 Practcial examples\nFor the practical part of this week, I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\nThese figures show this reflectance, e.g. band 5 produces the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels.\n\n\n\nSpectralReflectance"
  },
  {
    "objectID": "wk3.html#landsat-data",
    "href": "wk3.html#landsat-data",
    "title": "3  Week 3",
    "section": "3.1 Landsat Data",
    "text": "3.1 Landsat Data\nComposed of different satellites; Landsat 1-9\nOriginal Multispectral Camera (MSS) proposed by Virginia Norwood, wiht innovative design, whisk broom, not push broom. Pivoting mirror not scanner. NASA doubted digital camera originally, wanted RBV, limited to b, g, r, near-infrred."
  },
  {
    "objectID": "wk3.html#two-types-of-sensors",
    "href": "wk3.html#two-types-of-sensors",
    "title": "3  Week 3",
    "section": "3.2 Two types of sensors:",
    "text": "3.2 Two types of sensors:\n\nRBV:\nMSS:"
  },
  {
    "objectID": "wk3.html#data",
    "href": "wk3.html#data",
    "title": "3  Week 3",
    "section": "3.2 Data",
    "text": "3.2 Data\nData was made publicly available in 2008"
  },
  {
    "objectID": "wk3.html#corrections",
    "href": "wk3.html#corrections",
    "title": "3  Week 3",
    "section": "3.1 Corrections",
    "text": "3.1 Corrections\nThere are several issues with raw satellites imagery that require their corrections before using in analysis. Nowadays, the images are already corrected in most cases, but it’s good to understand what types of corrections need to be applied, and why. One example is a correction due to scan lines - gaps between images, which need to be smoothed.\n\n3.1.1 Geometric correction\nThis is a correction that is needed for images taken at off-nadir angle, meaning they are not looking directly down. We can imagine that the topography of the land, especially if there are mountains would significantly impact what the image shows when taken at an angle. The correction is done by mapping the points on a pre-correction image to the same points in the ground truth image. The correction can be done with OLS regression between the point values from the gold standard and the distorted image, where we’re looking to minimise the RMSE. There are two methods for this - input to output (forwrad mapping) and output to input (backward mapping). The latter is usually the default, and is done by mapping pixels from the distorted image pixel onto the gold standard image. This correction is basically like georeferencing an image, which does not have coordinates.\n\n\n3.1.2 Atmospheric correction\nThis correction is needed when the illumination source - sun going though the atmosphere is distorted resulting in haze and scattering in the images. This shows up in images as the adjacency effect, where pixel values switch their position. One method to deal with it is dark object subtraction, where the value of the darkest pixel is subtracted from the whole image. Second option is the pseudo-invariant feature.\nThere are three main types for this corrcetion: - absolute correction, where the atmposhpere is modelled with atmospheric radiative transfer models and controlled for/\n\nempirical correction type, that involves taking measurements on earth, which are used as ground truth to fit a linear regression model to minimise distortion.\n\nAlso worth mentioning is that the path radiance can be disturbed in different ways. Firstly radiance can get reflected in the atmosphere, scattering above the surface. Secondly, the light can get absorbed by the particles in the atmosphere, which results in not reahing the sensor, this is atmospheric attenuation.\nThe correction for this can be done first on several points in order to estimate the coefficients, which are then used to apply the correction apply to the rest of the image.\n\n\n3.1.3 Orthorectification / topographic correction\nThis is used to force pixels to be viewed at nadir angle. To correct for this, we can use a cosine correction, which requires information on the sensor geometry and an elevation model of the area imaged.\n\n\n3.1.4 Radiometric Calibration\nThis is not a correction but a calibration that converts the raw pixel values of an image from digital numbers to reflectance. In raw images, the pixels have image brightness or radiance recorded as a Digital Number (DN), which represents the intensity of the electromagnetic radiation per pixel. The difference between radiance and reflectance is that the former is amount of light recorded by the sensor, and the latter is the ratio of signal reflected from an object, to the amount recorded by the sensor.\nThus, radiometric Calibration is converting DN to spectral radiance, calculated using values that the sensor was calibrated for, before deployed on satellites.\nIn practice, most of the data we use, comes already analysis-ready. In Landsat algorithms like LEDPAS or L8SR apply the relevant corrections, and they are the Level 2 product."
  },
  {
    "objectID": "wk3.html#enhancements-and-data-joining",
    "href": "wk3.html#enhancements-and-data-joining",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.2 Enhancements and data joining",
    "text": "3.2 Enhancements and data joining\nThis section describes some manipulations that can be done on the corrected images. Firstly, we may need to join datasets as images naturally come in tiles. The edges of such tiles will usually overlap, in which case we can blend images into each other by mosaicking, or feathering.\n\n3.2.1 Filters\nApplying filters to an image is like applying a convolution kernel to an image. Different filters can be used to enhance, extract or smooth certain features of the image.\n####Texture Various operations can be done with filters, e.g. mean, variance, entropy and other operations are done by calculating probability, by dividing the values by the number of pixels in the kernel. The value of every pixel will then reflect, e.g. the variance of the kernel centered on that pixel.\nApplying the filters on the 4th band of the image of Dunedin from previous weeks, with a 7x7 window produced the measures shown here:\n Some of the measures may be difficult to interpret, as this was run on the band 4 representing infra-red - e.g. water is only distinctively shown in the mean texture, while high entropy is for example along the coastline.\nAnalysis will be updated.\nWe can also stack images on top of each other, e.g. from different dates, which is data fusion.\n\n3.2.1.1 PCA\nJust how PCA, a dimensionality reduction method can be used on tabular data, where the number of variables is reduced, so is it the same for satellite imagery. Linear combinations of multiple bands are combined in a way that maximises the variation in pixel values.\nAs part of the practical this method was applied on the Landsat data of Dunedin from previous weeks. The resulting principal components are presented in the figure below:\n\n\n\nPCA"
  },
  {
    "objectID": "wk1.html#practical-examples",
    "href": "wk1.html#practical-examples",
    "title": "1  Week 1",
    "section": "1.2 Practical examples",
    "text": "1.2 Practical examples\nFor the practical part of this week, I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\nThese figures show this reflectance, e.g. band 5 has the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels. These values, of course depend on the polygons that were selected.\n\n\n\nSpectralReflectance"
  },
  {
    "objectID": "wk1.html#applications",
    "href": "wk1.html#applications",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nThere are of course countless examples of research that uses remote sensing data. While browsing the literature I came across a project focused on using such data from different sources including Landsat to model the historic Aboriginal foraging habitats in the Australian Western Desert. This was done by assessing water accessibility, vegetation greenness, as well as the land topography, identifying locations where precontact Aboriginal people likely lived (Law et al. 2021). I find projects like this fascinating, as they show that remote sensing data enables us to study topics which would otherwise be impossible or very difficult to understand.\nStudies highlight the difficulties and the importance of certain consideration when working with data from satellites. For example, (Li, Saphores, and Gillespie 2015) use remote sensing data at two spatial resolutions - 30m of NDVI index and 0.6m for land cover classification, in order to investigate the economic benefits of urban green space. They show that although the NDVI data is more relevant, the higher resolution land cover is able to more precisely characterise this relationship, which shows the importance of high resolution data, especially when working at a city-scale.\nThe consideration of spectral resolutions for research likewise is very relevant. By using hyperspectral images at high spatial resolution (Taherzadeh and Shafri 2011) have been able to develop a model for classifying urban materials in Kuala Lumpur. The authors used a Support Vector Machine for this, showing its different predictive ability for different urban materials. The study also then used a Lee filter method to smooth the image, which improved the model’s predictions."
  },
  {
    "objectID": "wk1.html#reflections",
    "href": "wk1.html#reflections",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nThe practical part of this week was focused on getting started with downloading satellite imagery, both Landsat and Sentinel, and familiarising ourselves with the software, which can be used to analyse it. I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\n\n\n\nSpectral Reflectance\n\n\nThe figures above show, e.g. that band 5 has the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels. These values, of course depend on the polygons that were selected.\nHaving a practical example of a satellite image, was very helpful to realise what this data looks like and how it can be analysed. It was certainly an important realisation that working with this single, relatively small image requires a lot of memory for processing, which made me appreciate more the abilities of cloud computing for remote sensing data, especially when working on a larger scale, and with higher spatial and spectral resolutions data.\nWhen thinking about the vastness of available satellite imagery data, it’s easy to get overwhelmed. Before beginning to use such data for research questions, I definitely feel that it’s good to know how these sensors collect the data, why there are different bands, what they represent etc. As working with such data locally is naturally, rather difficult due to their size and computational expense needed to process them, I am curious to learn more about the cloud tools like GEE.\n\n\n\n\nLaw, W Boone, Peter Hiscock, Bertram Ostendorf, and Megan Lewis. 2021. “Using Satellite Imagery to Evaluate Precontact Aboriginal Foraging Habitats in the Australian Western Desert.” Scientific Reports 11 (1): 10755.\n\n\nLi, Wei, Jean-Daniel M Saphores, and Thomas W Gillespie. 2015. “A Comparison of the Economic Benefits of Urban Green Spaces Estimated with NDVI and with High-Resolution Land Cover Data.” Landscape and Urban Planning 133: 105–17.\n\n\nTaherzadeh, Ebrahim, and Helmi ZM Shafri. 2011. “Using Hyperspectral Remote Sensing Data in Urban Mapping over Kuala Lumpur.” In 2011 Joint Urban Remote Sensing Event, 405–8. IEEE."
  },
  {
    "objectID": "wk3.html#applications",
    "href": "wk3.html#applications",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nThe applications of the methods topics covered this week are also numerous. As discussed, there are several types of corrections that are necessary, and many different ways to apply them. For example, one study evaluated different models of atmospheric corrections on raw Landsat data, in order to find the most appropriate ones for the coastal environment around Hong Kong (Nazeer, Nichol, and Yung 2014). The authors found that different models for correction perform best over different surfaces. They also point to the fact that local characteristics of the atmosphere around Hong Kong have an impact on how the correction model performs, as it was created for continental landmass areas, this shows the importance of using a correction model appropriate for the local area.\nTexture-based methods applied to images have largely been used to identify or classify certain features of the environment. One such example is a study by (Kit, Lüdeke, and Reckien 2012), that identified slums in Hyderabad using texture-based methods on satellite imagery. The detection relies on a line detection based on lacunarity calculation, which is a measure that uses a variance filter. The performance of this method is compared to a PCA calculation from multiple bands, and the line detection method is found to be a significantly better predictor of slums in this city.\nThis methodology of feature engineering with filters and texture-based methods can often be used as input to machine learning models in order to train a model for classifying features. For example, (Tuia et al. 2009) have used SVMs for classifying high resolution urban images. The authors point to the fact that such methods, though, are limited as they do not consider the spatial relation between pixel values. It was shown that by including the spatial information, the model’s classification accuracy improved significantly."
  },
  {
    "objectID": "wk3.html#glossary-of-terms",
    "href": "wk3.html#glossary-of-terms",
    "title": "3  Week 3",
    "section": "3.4 Glossary of terms",
    "text": "3.4 Glossary of terms\nUsing collection 2, surface reflactance level 2, tier 1."
  },
  {
    "objectID": "wk3.html#sensor-types",
    "href": "wk3.html#sensor-types",
    "title": "3  Week 3",
    "section": "3.1 Sensor types",
    "text": "3.1 Sensor types\nFirstly, a bit about the history of sensors - the first digital sensor put on a Landsat satellite was the Multispectral Scanner Camera (MSS) built by Virginia Norwood. Its innovative design that used a whisk broom, rather than a push broom, was initially reluctantly implemented by NASA, as previous RBV sensors, were limited to only a few bands. The data captured by the satellites was made publicly available in 2008.\n—Pivoting mirror not scanner."
  },
  {
    "objectID": "wk3.html#reflections",
    "href": "wk3.html#reflections",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nThe more technically-heavy material of this week has given me a good grounding in understanding some of the aspects of satellite data that we don’t directly deal with, as data comes analysis-ready. What I find particularly interesting are the kernel-based operations that can be applied to satellite data and what features they might show. I had not thought about applying kernel-based operations to images as filters and using such outputs as possible features that could show us something useful.\nI had a go at applying some filters on the 4th band of the image of Dunedin from previous weeks. The images below are the outputs of applying a 7x7 window and calculating the mean, variance, entropy etc.\n\n\n\nTexture measures of Dunedin\n\n\nSome of the measures may be difficult to interpret, as this was run on the band 4 representing infra-red - e.g. water is only distinctively shown in the mean texture, while high entropy, for example, shows up along the coastline. I then applied the PCA algorithm on the original bands of the data, which gave the principal components visible below. \nOverall, I found the material this week particularly interesting, especially about filters and kernel based operations done on the images.\n\n\n\n\nKit, Oleksandr, Matthias Lüdeke, and Diana Reckien. 2012. “Texture-Based Identification of Urban Slums in Hyderabad, India Using Remote Sensing Data.” Applied Geography 32 (2): 660–67.\n\n\nNazeer, Majid, Janet E Nichol, and Ying-Kit Yung. 2014. “Evaluation of Atmospheric Correction Models and Landsat Surface Reflectance Product in an Urban Coastal Environment.” International Journal of Remote Sensing 35 (16): 6271–91.\n\n\nTuia, Devis, Frédéric Ratle, Alexei Pozdnoukhov, and Gustavo Camps-Valls. 2009. “Multisource Composite Kernels for Urban-Image Classification.” IEEE Geoscience and Remote Sensing Letters 7 (1): 88–92."
  },
  {
    "objectID": "wk4.html#policy-discussions",
    "href": "wk4.html#policy-discussions",
    "title": "4  Week 4",
    "section": "4.1 Policy discussions",
    "text": "4.1 Policy discussions"
  },
  {
    "objectID": "wk4.html#policy-applicationns",
    "href": "wk4.html#policy-applicationns",
    "title": "4  Week 4",
    "section": "4.1 Policy applicationns",
    "text": "4.1 Policy applicationns\nLand Use and land cover - difference - physicla propery and what people use it for."
  },
  {
    "objectID": "wk1.html#summary",
    "href": "wk1.html#summary",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nIn the first week of the module we were introduced to remote sensing, focusing on what satellite imagery is and how it iss collected. The first classification of sensor types, is into passive and active sources, the former use the sun’s energy reflected off the ground, while the latter use their own energy source to send signal to Earth and record it (e.g. SAR). The sensors record the signal by capturing electromagnetic waves, which can have different frequencies, enabling sensors to collect information about things that are not visible to human eye, due to varying spectral signatures of different objects. The figure below shows this spectrum of wavelengths.\n\n\n\nElectromagnetic spectrum. Image source: SkyWatch.com\n\n\nWe learned there are four resolutions of images captured by satellites - spatial, spectral, radiometric and temporal. Individual bands can be merged to create indices, which show specific characteristics of the Earth’s surface that we want to measure. In a situation where bands are not in the same spatial resolutions, images need to be resampled into the same resolution, in order to be able to work with such data conjointly, either by downscaling or upscaling.\nSatellite data, e.g. from Sentinel and Landsat satellites can be freely accessed from the Copernicus Open Access Hub. Software tools like QGIS and SNAP (specifically for Sentinel data) can be used to browse and analyse this data, e.g. for combining different bands to produce desired indices. Examples of such indices include false colour composite, the tasseled cap or atmospheric penetration composite, which allow us to visualise features of the environment that are not observable to the human eye."
  },
  {
    "objectID": "wk3.html#summary",
    "href": "wk3.html#summary",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThe first digital sensor put on a Landsat satellite was the Multispectral Scanner Camera (MSS) built by Virginia Norwood. Its innovative design was initially reluctantly implemented by NASA - the previous RBV sensors were limited to only a few bands. The data captured by the satellites began to be made publicly available in 2008.\n\n3.1.1 Corrections\nRaw satellite imagery require certain corrections before being ready for use in analysis. Even though, images come to us analysis-ready in most cases, it’s good to understand what types of corrections need to be applied, and why.\nGeometric correction This is a correction that is needed for images taken at off-nadir angle, meaning they are not looking directly down. The correction is done by mapping the points on a pre-correction image to the same points in the ground truth image, which can be done with simple OLS regression. This correction is basically like georeferencing an image without coordinates.\nAtmospheric correction This correction is needed when the signal going though the atmosphere is distorted resulting in haze and scattering in the images. This shows up in images as the adjacency effect, where pixel values switch their position. Also worth mentioning is that the radiance can be disturbed in different ways, e.g. it can scatter in the atmosphere, or the light can get absorbed by the particles in the atmosphere, which is atmospheric attenuation.\nOrthorectification / topographic correction This is used to force pixels to be viewed at nadir angle. To correct for this, we can use a cosine correction, which requires information on the sensor geometry and an elevation model of the area imaged.\n\n\n3.1.2 Radiometric Calibration\nRaw pixel values of a satellite image need to be converted from digital numbers to represent reflectance. In raw images, a Digital Number (DN) pixels represent image brightness or radiance recorded so the intensity of the electromagnetic radiation per pixel. Thus radiance is the amount of light recorded by the sensor, and reflectance is the ratio of signal reflected from an object, to the amount recorded by the sensor. Radiometric calibration is this process of converting DN to spectral radiance, calculated using values that the sensor was calibrated for, before deployed on satellites.\nAs mentioned, in practice, most of the data we use comes already analysis-ready. For Landsat, algorithms like LEDPAS or L8SR apply the relevant corrections, which are the Level 2 products.\n\n\n3.1.3 Enhancements and data joining\nCertain manipulations can be done of the corrected images. Firstly, we may need to join datasets as images naturally come in tiles. The edges of such tiles will usually overlap, in which case we can blend them into each other by mosaicking, or feathering.\nFilters Applying filters to an image is like applying a convolution kernel to an image. Different filters can be used to enhance, extract or smooth certain features of the image.\nTexture Various operations can be done with filters, e.g. calculating mean, variance, entropy and other operations of images. The value of every pixel will then reflect, e.g. the variance of the kernel centered on that pixel.\nWe can also stack images on top of each other, e.g. from different dates, which is data fusion.\nPCA Just how PCA, a dimensionality reduction method can be used on tabular data, where the number of variables is reduced, so is it the same for satellite imagery. Linear combinations of multiple bands are combined in a way that maximises the variation in pixel values thus reducing the spectral resolution."
  },
  {
    "objectID": "wk4.html#options-from-measuring",
    "href": "wk4.html#options-from-measuring",
    "title": "4  Week 4 - Policy applications",
    "section": "4.1 Options from measuring",
    "text": "4.1 Options from measuring\n\n4.1.1 Backscatter - problem affecting this imager coz of\n\nCorner reflections in urban environments\nShadowing - building behind another not imaged\nSpeckle - grainy, from scattering on ground - “salt and pepp\n\n\n\n4.1.2 Amplitude dataset\nWind affects detection of water bodeis, due to the water surface not being even\n\n\n4.1.3 Phase difference data\ndetecting shifts, elevationn models, eqarthqaukes unavailable in gee"
  },
  {
    "objectID": "wk4.html#aim",
    "href": "wk4.html#aim",
    "title": "4  Week 4 - Policy applications",
    "section": "4.2 Aim",
    "text": "4.2 Aim\nInforming policies with research ### Key global policy documents - directions New urban Agnda SDGs Local policy documents City mastrplans Who’s setting the policy and responsible for policy implementation"
  },
  {
    "objectID": "wk4.html#applications",
    "href": "wk4.html#applications",
    "title": "4  Week 4 - Policy applications",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nTherefore, in order to address this issue from both sides, policies would need to be developed that for one identify the most important areas of farmlands that would need to be protected and secondly identify inner-city areas where densification is most suitable, such as around the now under-construction Melbourne Suburban Rail Loop, at whose new stations, densification is due to be prioritised. This is where remote sensing data could help.\nUsing satellite imagery we could identify the most valuable areas which should be protected. The identification of soil quality and the valuation of natural environments are applications where remote sensing has been used (hazeu2014high?). Furthermore for finding areas of densification, remote sensing might not be an obvious choice, but could help in assessing the greenness of such areas or e.g. their soil permeability - all aspects that are important in order for the city to build dense urban spaces that are good for the environment."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "CASA0023",
    "section": "About me",
    "text": "About me\nTo be added"
  },
  {
    "objectID": "wk4.html#summary",
    "href": "wk4.html#summary",
    "title": "4  Week 4 - Policy applications",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nI recently read an article, which made me think about a problem that the city of Melbourne is facing, and that could potentially benefit from analysis of remote sensing data (Wilson 2024). The issue described in the article is about the land on the city’s fringe being converted from very prolific and important farmland into low-rise suburban developments. The land surrounding the city is actually some of the richest and most productive soils in the state of Victoria, which is important for both food security and the economy of the region. Such developments have also been found to increase fire risk, which is a very important issue in Australia (Stanley and March 2020).\nThe issue is that there is a lack of planning and policies in Melbourne that protect the valuable farmlands being used for developments. However, it’s important to realise that banning new developments in Melbourne is not a good idea, due to the city’s very unaffordable housing market. Curbing the supply of new housing would put even more pressure on the already strained and undersupplied property market. That is why it is important the city allows for more urban intensification, which is a growing topic in the city that more policies are starting to build towards. Therefore, there is the issue from two sides - one protecting the existing farmlands, and secondly building more affordable housing in the city."
  },
  {
    "objectID": "wk4.html#reflections",
    "href": "wk4.html#reflections",
    "title": "4  Week 4 - Policy applications",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nSo far I have not found any policies that make us of such data. The data that could be used for this application, would need to be of rather high spatial resolution, as this is an issue relevant to a particular city. This is because, what we’d be looking to achieve with such data requires the analysis of low-level features in the urban environment.\nMore discussion will be added\n\n\n\n\nStanley, Janet, and Alan March. 2020. “Black Saturday: Urban Sprawl and Climate Change Remain Key Dangers.” Pursuit. https://pursuit.unimelb.edu.au/articles/black-saturday-urban-sprawl-and-climate-change-remain-key-dangers.\n\n\nWilson, Katherine. 2024. “Guardian News & Media Produces a Variety of Content with Funding from Outside Parties.” The Guardian, May. https://www.theguardian.com/info/2017/may/22/guardian-news-media-funding-and-investment."
  }
]