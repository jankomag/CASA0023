[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023 Learning Diary",
    "section": "",
    "text": "Welcome to my CASA0023 Learning Diary\n\nRemotely Sensing Cities and Environments\nHere, I share my journey through the module CASA0023 Remotely Sensing Cities and Environments - as part of the Urban Spatial Science MSc degree in CASA, UCL (2023-24). Follow along, as I learn new techniques, review relevant research, apply the techniques in practice and reflect on the topics covered in each week.\n\n\nBackground\nI’m Jan, and I took this course, as I am curious to learn more about using aerial images and raster data from satellites for geospatial analysis. My experience with remote sensing, prior to taking this course was limited to a few introductory lectures about satellites and SAR imagery in a Geophysics module in my undergraduate. That is why I had a particular interest in taking this course to learn how these massive amounts of data (so much of it is captured everyday!), can be used to extract valuable insights to address global issues.\n\n\n\nMe before taking this class"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wk1.html",
    "href": "wk1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This week we were introduced to remote sensing, focusing on how satellite imagery is collected. Firstly, there are two types of sources, passive and active - which use their own energy source to monitor Earth (e.g. SAR). The signal is recorded by capturing electromagnetic waves, which can have different frequencies, enabling sensors to find collect information about things that are not visible to human eye, because different objects have different spectral signature."
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "2  Xaringan Presentation on a sensor",
    "section": "",
    "text": "This week, we were introduced to Xaringan - a tool for making presentations in code.\nBelow is my presentation on Landsat 9"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chen, D, DA Stow, and P Gong. 2004. “Examining the Effect of\nSpatial Resolution and Texture Window Size on Classification Accuracy:\nAn Urban Environment Case.” International Journal of Remote\nSensing 25 (11): 2177–92.\n\n\nDuro, Dennis C, Steven E Franklin, and Monique G Dubé. 2012. “A\nComparison of Pixel-Based and Object-Based Image Analysis with Selected\nMachine Learning Algorithms for the Classification of Agricultural\nLandscapes Using SPOT-5 HRG Imagery.” Remote Sensing of\nEnvironment 118: 259–72.\n\n\nGoldblatt, Ran, Wei You, Gordon Hanson, and Amit K Khandelwal. 2016.\n“Detecting the Boundaries of Urban Areas in India: A Dataset for\nPixel-Based Image Classification in Google Earth Engine.”\nRemote Sensing 8 (8): 634.\n\n\nHazeu, Gerard, Pavel Milenov, Bas Pedroli, Vessela Samoungi, Michiel Van\nEupen, and Vassil Vassilev. 2014. “High Nature Value Farmland\nIdentification from Satellite Imagery, a Comparison of Two\nMethodological Approaches.” International Journal of Applied\nEarth Observation and Geoinformation 30: 98–112.\n\n\nKajimoto, Muneyoshi, and Junichi Susaki. 2013. “Urban Density\nEstimation from Polarimetric SAR Images Based on a POA Correction\nMethod.” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing 6 (3): 1418–29.\n\n\nKarasiak, Nicolas, J-F Dejoux, Claude Monteil, and David Sheeren. 2022.\n“Spatial Dependence Between Training and Test Sets: Another\nPitfall of Classification Accuracy Assessment in Remote Sensing.”\nMachine Learning 111 (7): 2715–40.\n\n\nKit, Oleksandr, Matthias Lüdeke, and Diana Reckien. 2012.\n“Texture-Based Identification of Urban Slums in Hyderabad, India\nUsing Remote Sensing Data.” Applied Geography 32 (2):\n660–67.\n\n\nLaw, W Boone, Peter Hiscock, Bertram Ostendorf, and Megan Lewis. 2021.\n“Using Satellite Imagery to Evaluate Precontact Aboriginal\nForaging Habitats in the Australian Western Desert.”\nScientific Reports 11 (1): 10755.\n\n\nLi, Wei, Jean-Daniel M Saphores, and Thomas W Gillespie. 2015. “A\nComparison of the Economic Benefits of Urban Green Spaces Estimated with\nNDVI and with High-Resolution Land Cover Data.” Landscape and\nUrban Planning 133: 105–17.\n\n\nLiu, Wen, Masashi Matsuoka, Fumio Yamazaki, Takashi Nonaka, and Tadashi\nSasagawa. 2013. “Detection of Surface Displacements and Liquefied\nAreas in the 2011 Christchurch Earthquake from SAR Data.”\n\n\nLiu, Xiaoping, Guohua Hu, Yimin Chen, Xia Li, Xiaocong Xu, Shaoying Li,\nFengsong Pei, and Shaojian Wang. 2018. “High-Resolution\nMulti-Temporal Mapping of Global Urban Land Using Landsat Images Based\non the Google Earth Engine Platform.” Remote Sensing of\nEnvironment 209: 227–39.\n\n\nNazeer, Majid, Janet E Nichol, and Ying-Kit Yung. 2014.\n“Evaluation of Atmospheric Correction Models and Landsat Surface\nReflectance Product in an Urban Coastal Environment.”\nInternational Journal of Remote Sensing 35 (16): 6271–91.\n\n\nOort, Pepijn AJ van, Arnold K Bregt, Sytze de Bruin, Allard JW de Wit,\nand Alfred Stein. 2004. “Spatial Variability in Classification\nAccuracy of Agricultural Crops in the Dutch National Land-Cover\nDatabase.” International Journal of Geographical Information\nScience 18 (6): 611–26.\n\n\nSeydi, Seyd Teymoor, Mehdi Akhoondzadeh, Meisam Amani, and Sahel\nMahdavi. 2021. “Wildfire Damage Assessment over Australia Using\nSentinel-2 Imagery and MODIS Land Cover Product Within the Google Earth\nEngine Cloud Platform.” Remote Sensing 13 (2): 220.\n\n\nShackelford, Aaron K, and Curt H Davis. 2003. “A Combined Fuzzy\nPixel-Based and Object-Based Approach for Classification of\nHigh-Resolution Multispectral Data over Urban Areas.” IEEE\nTransactions on GeoScience and Remote Sensing 41 (10): 2354–63.\n\n\nStanley, Janet, and Alan March. 2020. “Black Saturday: Urban\nSprawl and Climate Change Remain Key Dangers.” Pursuit.\nhttps://pursuit.unimelb.edu.au/articles/black-saturday-urban-sprawl-and-climate-change-remain-key-dangers.\n\n\nSuzuki, Teppei, Shuichi Akizuki, Naoki Kato, and Yoshimitsu Aoki. 2018.\n“Superpixel Convolution for Segmentation.” In 2018 25th\nIEEE International Conference on Image Processing (ICIP), 3249–53.\nIEEE.\n\n\nTaherzadeh, Ebrahim, and Helmi ZM Shafri. 2011. “Using\nHyperspectral Remote Sensing Data in Urban Mapping over Kuala\nLumpur.” In 2011 Joint Urban Remote Sensing Event,\n405–8. IEEE.\n\n\nTripathy, Pratyush, and Teja Malladi. 2022. “Global Flood Mapper:\nA Novel Google Earth Engine Application for Rapid Flood Mapping Using\nSentinel-1 SAR.” Natural Hazards 114 (2): 1341–63.\n\n\nTuia, Devis, Frédéric Ratle, Alexei Pozdnoukhov, and Gustavo\nCamps-Valls. 2009. “Multisource Composite Kernels for Urban-Image\nClassification.” IEEE Geoscience and Remote Sensing\nLetters 7 (1): 88–92.\n\n\nVelastegui-Montoya, Andrés, Néstor Montalván-Burbano, Paúl Carrión-Mero,\nHugo Rivera-Torres, Luı́s Sadeck, and Marcos Adami. 2023. “Google\nEarth Engine: A Global Analysis and Future Trends.” Remote\nSensing 15 (14): 3675.\n\n\nVictoria State Government. 2017. “Plan Melbourne\n2017–2050.” 2017. https://www.planning.vic.gov.au/guides-and-resources/strategies-and-initiatives/plan-melbourne.\n\n\nWilson, Katherine. 2024. “Guardian News & Media Produces a\nVariety of Content with Funding from Outside Parties.” The\nGuardian, May. https://www.theguardian.com/info/2017/may/22/guardian-news-media-funding-and-investment.\n\n\nWu, Qiusheng. 2020. “Geemap: A Python Package for Interactive\nMapping with Google Earth Engine.” Journal of Open Source\nSoftware 5 (51): 2305.\n\n\nYIMBY Melbourne. 2023. “Melbourne’s Missing\nMiddle in 2023.” Melbourne, Australia: YIMBY Melbourne;\nPresentation."
  },
  {
    "objectID": "index.html#casa0023-learning-diary",
    "href": "index.html#casa0023-learning-diary",
    "title": "CASA0023",
    "section": "CASA0023 Learning Diary",
    "text": "CASA0023 Learning Diary\nThis is the Portfolio for CASA0023 Remotely Sensing Cities and Environments, created with Quarto."
  },
  {
    "objectID": "wk1.html#introduction-to-remote-sensing",
    "href": "wk1.html#introduction-to-remote-sensing",
    "title": "1  Week 1",
    "section": "1.1 Introduction to Remote Sensing",
    "text": "1.1 Introduction to Remote Sensing\nIn the first week of the module we were introduced to remote sensing, focusing on the principles of what satellite imagery is and how it’s collected. The first classification of sensor types, is into passive and active sources, the former use the sun’s energy reflected off the ground, while the latter use their own energy source to send signal to Earth and record it (e.g. SAR). The sensors record the signal by capturing electromagnetic waves, which can have different frequencies, enabling sensors to collect information about things that are not visible to human eye, due to varying spectral signatures of different objects. The figure below shows such a spectrum of wavelengths that signal can take, showing a rather small part that is actually visible.\n\n\n\nElectromagnetic spectrum. Image source: SkyWatch.com\n\n\nThere are certain important considerations that we need to be aware of about the signal that is being collected by the satellites. Firstly, as the signal travels, it passes through the atmosphere, which can interrupt it causing atmospheric haze of various types. Furthermore, because smaller wavelengths scatter easier, they are more difficult to pick up by the sensor.\n\n1.1.1 The Four Resolutions\nThere are four resolutions of images captured by satellites. Firstly, spatial - which tells us how spatially precise an image is, e.g. 10m per px. Spectral resolution refers to the bands which have been recorded, as not all wavelengths are observable; images with many bands are referred to as multispectral or hyperspectral. Radiometric resolution describes the sensitivity e.g. 8-bit; and finally the temporal resolution is how often the certain place on Earth is photographed.\n\n\n1.1.2 Merging bands\nIndividual bands can be merged to create indices, which show specific characteristics of the Earth’s surface we want to measure. In a situation where bands are not in the same spatial resolutions, images need to be resampled into the same resolution, in order to be able to work with such data conjointly. There are two options for this: downscaling - going from a higher resolution like 10m to a lower like 30m; or upscaling, which is the opposite operation. The former is preferred in most cases, as the transformation makes fewer assumptions. An example of an index that can be computed by merging specific bands in a certain way is the tasseled cap transformation, which is calculated from the bands representing brightness, greenness and wetness. The resulting index can be used to identify vegetation, as well as urban areas.\n\n\n1.1.3 Working with sattelite imagery data\nSatellite data taken by Sentinel satellites can be freely accessed from the Copernicus Open Access Hub. Software tools like QGIS and SNAP (specifically for Sentinel data) can be used to browse and analyse this data, e.g. for combining different bands to produce desired indices. Examples of such indices include false colour composite, or atmospheric penetration composite, which allow us to visualise features of the environment that are not visible to the human eye. Landsat images, which are free global images of temporal resolution 16 days, can also be acquired from the USGS website."
  },
  {
    "objectID": "wk1.html#practcial-examples",
    "href": "wk1.html#practcial-examples",
    "title": "1  Week 1",
    "section": "1.2 Practcial examples",
    "text": "1.2 Practcial examples\nFor the practical part of this week, I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\nThese figures show this reflectance, e.g. band 5 produces the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels.\n\n\n\nSpectralReflectance"
  },
  {
    "objectID": "wk3.html#landsat-data",
    "href": "wk3.html#landsat-data",
    "title": "3  Week 3",
    "section": "3.1 Landsat Data",
    "text": "3.1 Landsat Data\nComposed of different satellites; Landsat 1-9\nOriginal Multispectral Camera (MSS) proposed by Virginia Norwood, wiht innovative design, whisk broom, not push broom. Pivoting mirror not scanner. NASA doubted digital camera originally, wanted RBV, limited to b, g, r, near-infrred."
  },
  {
    "objectID": "wk3.html#two-types-of-sensors",
    "href": "wk3.html#two-types-of-sensors",
    "title": "3  Week 3",
    "section": "3.2 Two types of sensors:",
    "text": "3.2 Two types of sensors:\n\nRBV:\nMSS:"
  },
  {
    "objectID": "wk3.html#data",
    "href": "wk3.html#data",
    "title": "3  Week 3",
    "section": "3.2 Data",
    "text": "3.2 Data\nData was made publicly available in 2008"
  },
  {
    "objectID": "wk3.html#corrections",
    "href": "wk3.html#corrections",
    "title": "3  Week 3",
    "section": "3.1 Corrections",
    "text": "3.1 Corrections\nThere are several issues with raw satellites imagery that require their corrections before using in analysis. Nowadays, the images are already corrected in most cases, but it’s good to understand what types of corrections need to be applied, and why. One example is a correction due to scan lines - gaps between images, which need to be smoothed.\n\n3.1.1 Geometric correction\nThis is a correction that is needed for images taken at off-nadir angle, meaning they are not looking directly down. We can imagine that the topography of the land, especially if there are mountains would significantly impact what the image shows when taken at an angle. The correction is done by mapping the points on a pre-correction image to the same points in the ground truth image. The correction can be done with OLS regression between the point values from the gold standard and the distorted image, where we’re looking to minimise the RMSE. There are two methods for this - input to output (forwrad mapping) and output to input (backward mapping). The latter is usually the default, and is done by mapping pixels from the distorted image pixel onto the gold standard image. This correction is basically like georeferencing an image, which does not have coordinates.\n\n\n3.1.2 Atmospheric correction\nThis correction is needed when the illumination source - sun going though the atmosphere is distorted resulting in haze and scattering in the images. This shows up in images as the adjacency effect, where pixel values switch their position. One method to deal with it is dark object subtraction, where the value of the darkest pixel is subtracted from the whole image. Second option is the pseudo-invariant feature.\nThere are three main types for this corrcetion: - absolute correction, where the atmposhpere is modelled with atmospheric radiative transfer models and controlled for/\n\nempirical correction type, that involves taking measurements on earth, which are used as ground truth to fit a linear regression model to minimise distortion.\n\nAlso worth mentioning is that the path radiance can be disturbed in different ways. Firstly radiance can get reflected in the atmosphere, scattering above the surface. Secondly, the light can get absorbed by the particles in the atmosphere, which results in not reahing the sensor, this is atmospheric attenuation.\nThe correction for this can be done first on several points in order to estimate the coefficients, which are then used to apply the correction apply to the rest of the image.\n\n\n3.1.3 Orthorectification / topographic correction\nThis is used to force pixels to be viewed at nadir angle. To correct for this, we can use a cosine correction, which requires information on the sensor geometry and an elevation model of the area imaged.\n\n\n3.1.4 Radiometric Calibration\nThis is not a correction but a calibration that converts the raw pixel values of an image from digital numbers to reflectance. In raw images, the pixels have image brightness or radiance recorded as a Digital Number (DN), which represents the intensity of the electromagnetic radiation per pixel. The difference between radiance and reflectance is that the former is amount of light recorded by the sensor, and the latter is the ratio of signal reflected from an object, to the amount recorded by the sensor.\nThus, radiometric Calibration is converting DN to spectral radiance, calculated using values that the sensor was calibrated for, before deployed on satellites.\nIn practice, most of the data we use, comes already analysis-ready. In Landsat algorithms like LEDPAS or L8SR apply the relevant corrections, and they are the Level 2 product."
  },
  {
    "objectID": "wk3.html#enhancements-and-data-joining",
    "href": "wk3.html#enhancements-and-data-joining",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.2 Enhancements and data joining",
    "text": "3.2 Enhancements and data joining\nThis section describes some manipulations that can be done on the corrected images. Firstly, we may need to join datasets as images naturally come in tiles. The edges of such tiles will usually overlap, in which case we can blend images into each other by mosaicking, or feathering.\n\n3.2.1 Filters\nApplying filters to an image is like applying a convolution kernel to an image. Different filters can be used to enhance, extract or smooth certain features of the image.\n####Texture Various operations can be done with filters, e.g. mean, variance, entropy and other operations are done by calculating probability, by dividing the values by the number of pixels in the kernel. The value of every pixel will then reflect, e.g. the variance of the kernel centered on that pixel.\nApplying the filters on the 4th band of the image of Dunedin from previous weeks, with a 7x7 window produced the measures shown here:\n Some of the measures may be difficult to interpret, as this was run on the band 4 representing infra-red - e.g. water is only distinctively shown in the mean texture, while high entropy is for example along the coastline.\nAnalysis will be updated.\nWe can also stack images on top of each other, e.g. from different dates, which is data fusion.\n\n3.2.1.1 PCA\nJust how PCA, a dimensionality reduction method can be used on tabular data, where the number of variables is reduced, so is it the same for satellite imagery. Linear combinations of multiple bands are combined in a way that maximises the variation in pixel values.\nAs part of the practical this method was applied on the Landsat data of Dunedin from previous weeks. The resulting principal components are presented in the figure below:\n\n\n\nPCA"
  },
  {
    "objectID": "wk1.html#practical-examples",
    "href": "wk1.html#practical-examples",
    "title": "1  Week 1",
    "section": "1.2 Practical examples",
    "text": "1.2 Practical examples\nFor the practical part of this week, I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\nThese figures show this reflectance, e.g. band 5 has the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels. These values, of course depend on the polygons that were selected.\n\n\n\nSpectralReflectance"
  },
  {
    "objectID": "wk1.html#applications",
    "href": "wk1.html#applications",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nThere are of course countless examples of research that uses remote sensing data. While browsing the literature I came across a project focused on using such data from different sources including Landsat to model the historic Aboriginal foraging habitats in the Australian Western Desert. This was done by assessing water accessibility, vegetation greenness, as well as the land topography, identifying locations where precontact Aboriginal people likely lived (Law et al. 2021). I find projects like this fascinating, as they show that remote sensing data enables us to study topics which would otherwise be impossible or very difficult to understand.\nStudies highlight the difficulties and the importance of certain consideration when working with data from satellites. For example, Li, Saphores, and Gillespie (2015) use remote sensing data at two spatial resolutions - 30m of NDVI index and 0.6m for land cover classification, in order to investigate the economic benefits of urban green space. They show that although the NDVI data is more relevant, the higher resolution land cover is able to more precisely characterise the studied relationship, which shows the importance of high resolution data, especially when working in urban scale.\nThe consideration of spectral resolutions for research likewise is very relevant. By using hyperspectral images at high spatial resolution Taherzadeh and Shafri (2011) have been able to develop a model for classifying urban materials in Kuala Lumpur. The authors used a Support Vector Machine for this, showing different predictive ability for different urban materials. The study also then used a Lee filter method to smooth the image, which improved the model’s predictions."
  },
  {
    "objectID": "wk1.html#reflections",
    "href": "wk1.html#reflections",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nI particularly enjoyed the practical part of this week’s material, where I got started with downloading satellite imagery, both Landsat and Sentinel, and familiarising myslef with the SNAP software. I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.\n\n\n\nLandsat image of Dunedin in SNAP\n\n\nI crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then brought this in R to examine the reflectance of these land use types for different bands.\n\n\n\nSpectral Reflectance\n\n\nThe figures above show, e.g. that band 5 has the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels. These values, of course depend on the polygons that were selected.\nHaving a practical example of a satellite image was very helpful to realise what this data looks like and how it can be analysed. It was certainly an important realisation that working even with a single, relatively small image, requires a lot of memory for processing, which made me appreciate more the abilities of cloud computing for remote sensing data, especially when working on a larger scale, and with higher spatial and spectral resolutions data.\nWhen thinking about the vastness of available satellite imagery data, it’s easy to get overwhelmed. Before beginning to use such data for research questions, I definitely feel that it’s good to know how these sensors collect the data, why there are different bands and what they represent. As working with such data locally is naturally, rather difficult due to their size and computational expense needed to process them, I am curious to learn more about the cloud tools like Google Earth Engine (see sec-week5).\n\n\n\n\nLaw, W Boone, Peter Hiscock, Bertram Ostendorf, and Megan Lewis. 2021. “Using Satellite Imagery to Evaluate Precontact Aboriginal Foraging Habitats in the Australian Western Desert.” Scientific Reports 11 (1): 10755.\n\n\nLi, Wei, Jean-Daniel M Saphores, and Thomas W Gillespie. 2015. “A Comparison of the Economic Benefits of Urban Green Spaces Estimated with NDVI and with High-Resolution Land Cover Data.” Landscape and Urban Planning 133: 105–17.\n\n\nTaherzadeh, Ebrahim, and Helmi ZM Shafri. 2011. “Using Hyperspectral Remote Sensing Data in Urban Mapping over Kuala Lumpur.” In 2011 Joint Urban Remote Sensing Event, 405–8. IEEE."
  },
  {
    "objectID": "wk3.html#applications",
    "href": "wk3.html#applications",
    "title": "3  Corrections and Data Enhancements",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nResearch utilising the methods covered this week is plentiful. As discussed, there are several types of corrections that are necessary, and many different ways to apply them. For example, one study evaluated different models of atmospheric corrections on raw Landsat data, in order to find the most appropriate ones for the coastal environment around Hong Kong (Nazeer, Nichol, and Yung 2014), and found various correction models to perform best over different land-cover classes. They also point to the fact that local characteristics of the atmosphere around Hong Kong have an impact on how the correction model performs, which matters especially when a model was created for continental landmass areas. This highlight the importance of using a correction model appropriate for the local area.\nOne interesting study by Chen, Stow, and Gong (2004) evaluated the classification accuracy for different window sizes and spatial resolutions of imagery. One interesting finding is that calculating texture measures and using them as input had a higher influence on improved model performance for high resolution data. The window size was more important for model accuracy for high resolution data.\nTexture-based methods applied to images have largely been used to identify or classify certain features of the environment. One such example is a study by (Kit, Lüdeke, and Reckien 2012), that identified slums in Hyderabad using texture-based methods on satellite imagery. The detection relies on a line detection based on lacunarity calculation, which is a measure that uses a variance filter. The performance of this method is compared to a PCA calculation from multiple bands, and the line detection method is found to be a viable method for predicting slums in this city.\nThis methodology of feature engineering with filters and texture-based methods can often be used as input to machine learning models in order to train a model for classifying features. For example, (Tuia et al. 2009) have used SVMs for classifying high resolution urban images. The authors point to the fact that such methods, though, are limited as they do not consider the spatial relation between pixel values. It was shown that by including the spatial information, the model’s classification accuracy improved significantly."
  },
  {
    "objectID": "wk3.html#glossary-of-terms",
    "href": "wk3.html#glossary-of-terms",
    "title": "3  Week 3",
    "section": "3.4 Glossary of terms",
    "text": "3.4 Glossary of terms\nUsing collection 2, surface reflactance level 2, tier 1."
  },
  {
    "objectID": "wk3.html#sensor-types",
    "href": "wk3.html#sensor-types",
    "title": "3  Week 3",
    "section": "3.1 Sensor types",
    "text": "3.1 Sensor types\nFirstly, a bit about the history of sensors - the first digital sensor put on a Landsat satellite was the Multispectral Scanner Camera (MSS) built by Virginia Norwood. Its innovative design that used a whisk broom, rather than a push broom, was initially reluctantly implemented by NASA, as previous RBV sensors, were limited to only a few bands. The data captured by the satellites was made publicly available in 2008.\n—Pivoting mirror not scanner."
  },
  {
    "objectID": "wk3.html#reflections",
    "href": "wk3.html#reflections",
    "title": "3  Corrections and Data Enhancements",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nThe more technically-heavy material of this week has given me a good grounding in understanding some of the aspects of satellite data that we don’t directly deal with, as data comes analysis-ready. What I find particularly interesting are the kernel-based operations that can be applied to satellite data and what features they might show. I previously had not encountered the methodology of applying kernel-based operations to images as feature extractors, which could then be used for modelling.\nI had a go at applying several filters to the 4th band of the image of Dunedin from previous weeks. The images below are the outputs of applying a 7x7 window and calculating the mean, variance, entropy etc.\n\n\n\nTexture measures of Dunedin\n\n\nSome of the measures may be difficult to interpret, as this was run on the band 4 representing green - e.g. water is only distinctively shown in the mean raster, while high entropy, for example, shows up along the coastline. I then applied the PCA algorithm on the original bands of the data, which gave the principal components visible below. \nOverall, I found the material this week particularly interesting, especially learning about filters and kernel based operations, which can be performed on these data\n\n\n\n\nChen, D, DA Stow, and P Gong. 2004. “Examining the Effect of Spatial Resolution and Texture Window Size on Classification Accuracy: An Urban Environment Case.” International Journal of Remote Sensing 25 (11): 2177–92.\n\n\nKit, Oleksandr, Matthias Lüdeke, and Diana Reckien. 2012. “Texture-Based Identification of Urban Slums in Hyderabad, India Using Remote Sensing Data.” Applied Geography 32 (2): 660–67.\n\n\nNazeer, Majid, Janet E Nichol, and Ying-Kit Yung. 2014. “Evaluation of Atmospheric Correction Models and Landsat Surface Reflectance Product in an Urban Coastal Environment.” International Journal of Remote Sensing 35 (16): 6271–91.\n\n\nTuia, Devis, Frédéric Ratle, Alexei Pozdnoukhov, and Gustavo Camps-Valls. 2009. “Multisource Composite Kernels for Urban-Image Classification.” IEEE Geoscience and Remote Sensing Letters 7 (1): 88–92."
  },
  {
    "objectID": "wk4.html#policy-discussions",
    "href": "wk4.html#policy-discussions",
    "title": "4  Week 4",
    "section": "4.1 Policy discussions",
    "text": "4.1 Policy discussions"
  },
  {
    "objectID": "wk4.html#policy-applicationns",
    "href": "wk4.html#policy-applicationns",
    "title": "4  Week 4",
    "section": "4.1 Policy applicationns",
    "text": "4.1 Policy applicationns\nLand Use and land cover - difference - physicla propery and what people use it for."
  },
  {
    "objectID": "wk1.html#summary",
    "href": "wk1.html#summary",
    "title": "1  Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nIn the first week of the module we were introduced to remote sensing, focusing on what satellite imagery is and how it’s collected. Firstly, there are two main types of sensors - passive and active, the former use the sun’s energy reflected off the ground, while the latter use their own energy source to send signal to Earth and record it (see sec-week9 for SAR). The sensors record the signal by capturing electromagnetic waves, which can have different frequencies, enabling sensors to collect information about things that are not visible to human eye, due to varying spectral signatures of different objects. The figure below shows a spectrum of possible wavelengths, with only a small window of visible wavelengths\n\n\n\nElectromagnetic spectrum. Image source: SkyWatch.com\n\n\nThere are four resolutions of images captured by satellites - spatial, spectral, radiometric and temporal. Individual bands can be merged to create indices, which show specific characteristics of the Earth’s surface that we want to measure. In a situation where bands are not in the same spatial resolutions, images need to be resampled into the same resolution, in order to be able to work with such data conjointly, either by downscaling or upscaling.\nSatellite data, e.g. from Sentinel and Landsat satellites can be freely accessed from the Copernicus Open Access Hub. Software tools like QGIS, GEE and SNAP (specifically for Sentinel data) can be used to analyse and process this data, e.g. for combining different bands to produce desired indices. Examples of such indices include false colour composite, the tasseled cap or atmospheric penetration composite, which allow us to visualise features of the environment that are not observable to the human eye."
  },
  {
    "objectID": "wk3.html#summary",
    "href": "wk3.html#summary",
    "title": "3  Corrections and Data Enhancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThis week, we start with a discussion on the history of satellite imagery. The first digital sensor put on a Landsat satellite was the Multispectral Scanner Camera (MSS) built by Virginia Norwood. Its innovative design was initially reluctantly implemented by NASA - the previous RBV sensors were limited to only a few bands. The image data captured by satellites was made publicly available in 2008.\n\n3.1.1 Corrections\nRaw satellite imagery require certain corrections before being used for analysis. Even though these days, images come to us analysis-ready in most cases, it’s good to understand what types of corrections need to be applied, and why.\nGeometric correction This is a correction of perspective that needs to be applied for images taken at off-nadir angle, meaning they are made not directly towards the Earth. The correction is done by mapping the points on a pre-correction image to the same points in the ground truth image, which can be done with simple OLS regression. This correction is a similar operation to georeferencing a map without coordinates.\nAtmospheric correction This correction is needed when the signal going though the atmosphere is distorted resulting in haze and scattering in the images. This shows up in images as the adjacency effect, where pixel values switch their positions. Also worth mentioning is that the radiance can be disturbed in different ways, e.g. it can scatter in the atmosphere, or the light can get absorbed by the particles in the atmosphere - atmospheric attenuation.\nOrthorectification / topographic correction This is used to force pixels to be viewed at nadir angle. To correct for this, we can use a cosine correction, which requires information on the sensor geometry and an elevation model of the area imaged.\n\n\n3.1.2 Radiometric Calibration\nRaw pixel values of a satellite image need to be converted from digital numbers to represent reflectance. In raw images, a Digital Number (DN) pixels represent image brightness or radiance recorded, which is the intensity of the electromagnetic radiation per pixel. Thus, radiance is the amount of light recorded by the sensor, while reflectance is the ratio of signal reflected from an object, to the amount recorded by the sensor. Radiometric calibration is this process of converting DN to spectral radiance, calculated using values that the sensor was calibrated for, before deployed on satellites.\nAs mentioned, in practice, most of the data we use comes already analysis-ready. For Landsat, algorithms like LEDPAS or L8SR apply the relevant corrections, which are then made into e.g. Level 2 products.\n\n\n3.1.3 Enhancements and data joining\nCertain manipulations can be done of the corrected images. Firstly, we may need to join multiple, images that come in tiles. The edges of such tiles will usually overlap, in which case we can blend them into each other by mosaicking, or feathering.\nFilters filters are essential convolution kernels (moving window). Different filters can be used to enhance, or smooth the texture of images or extract certain features. Various operations can be done with filters, e.g. calculating mean, variance, entropy etc. The resulting data will then hold the values of the operation applied to the pixels within the kernel centered on every pixel.\nWe can also stack images on top of each other, e.g. from different dates, which is data fusion.\nJust how PCA, a dimensionality reduction method can be used on tabular data, where the number of variables is reduced, so is it the same for satellite imagery. Linear combinations of multiple bands are combined in a way that maximises the variation in pixel values thus reducing the spectral resolution."
  },
  {
    "objectID": "wk4.html#options-from-measuring",
    "href": "wk4.html#options-from-measuring",
    "title": "4  Week 4 - Policy applications",
    "section": "4.1 Options from measuring",
    "text": "4.1 Options from measuring\n\n4.1.1 Backscatter - problem affecting this imager coz of\n\nCorner reflections in urban environments\nShadowing - building behind another not imaged\nSpeckle - grainy, from scattering on ground - “salt and pepp\n\n\n\n4.1.2 Amplitude dataset\nWind affects detection of water bodeis, due to the water surface not being even\n\n\n4.1.3 Phase difference data\ndetecting shifts, elevationn models, eqarthqaukes unavailable in gee"
  },
  {
    "objectID": "wk4.html#aim",
    "href": "wk4.html#aim",
    "title": "4  Week 4 - Policy applications",
    "section": "4.2 Aim",
    "text": "4.2 Aim\nInforming policies with research ### Key global policy documents - directions New urban Agnda SDGs Local policy documents City mastrplans Who’s setting the policy and responsible for policy implementation"
  },
  {
    "objectID": "wk4.html#applications",
    "href": "wk4.html#applications",
    "title": "4  Policy applications",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nIn order to address this issue from both sides, policies would need to be developed that for one identify the most important areas of farmlands that would need to be protected and secondly identify inner-city areas where urban densification is most suitable. For example around the currently under-construction Melbourne Suburban Rail Loop stations where densification is being prioritised. This is where remote sensing data could help.\nUsing satellite imagery we could identify the most important farmlands, which should be prioritised over developments. The identification of soil quality and the valuation of natural environments are applications where remote sensing can be used (Hazeu et al. 2014). Furthermore for finding areas of densification, remote sensing might not be an obvious choice, but could help e.g. in assessing the greenness of urban areas or soil permeability - all important aspects in order for the city to densify sustainably."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "CASA0023",
    "section": "About me",
    "text": "About me\nTo be added"
  },
  {
    "objectID": "wk4.html#summary",
    "href": "wk4.html#summary",
    "title": "4  Policy applications",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nI recently read an article about an issue that the city of Melbourne is facing, which could potentially benefit from analysis of remote sensing data (Wilson 2024). The issue described in the article is about the land on the city’s fringes being converted from farmland into low-rise suburban developments. The land surrounding the city has some of the richest and most productive soils in the state of Victoria, which is important for both food security and the local, rural economy. The new developments encroaching onto this land are also associated with increased fire risk, which is a very important issue in this region of Australia (Stanley and March 2020).\nThere is a lack of policies in Melbourne that would address this issue and constrain where developments are allowed to happen, given the potential of the land for agriculture. Of course, completely halting new developments in Melbourne is a really bad idea, due to the city’s very unaffordable housing market and projected population growth. Curbing the supply of new housing would put even more pressure on the already strained property market. That is why it is important the city allows for more urban intensification, and designates areas where the city should be growing through densification, not just through sprawl. This is an increasingly discussed topic in Melbourne with a growing community of missing-middle advocates (YIMBY Melbourne 2023). Therefore, the need for more housing in Melbourne is a twofold problem - firstly protecting the existing farmlands on city’s fringes, and secondly building more affordable housing in the city in the form of apartments and townhouses."
  },
  {
    "objectID": "wk4.html#reflections",
    "href": "wk4.html#reflections",
    "title": "4  Policy applications",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nThe approach outlined above of using remotely sensed data for this issue, may not be the best choice for all aspects of the problem. Firstly, it would require using very high-resolution imagery to be able to identify features like buildings or individual trees. Secondly, for analysis of urban areas where densification potential is highest, a natural choice would be to rely on vector data, e.g. population data aggregated to spatial units, or transport and infrastructure data.\nThe Melbourne Plan 2017-2050 mentions in its implementation the importance of using spatial data for planning new developments and open spaces (Victoria State Government 2017). The plan even mentions in one section how the city will “protect the right to farm in key locations within green wedges and peri-urban areas”. However, the specific methodology or mention of remote sensing methods is lacking.\n\n\n\n\nHazeu, Gerard, Pavel Milenov, Bas Pedroli, Vessela Samoungi, Michiel Van Eupen, and Vassil Vassilev. 2014. “High Nature Value Farmland Identification from Satellite Imagery, a Comparison of Two Methodological Approaches.” International Journal of Applied Earth Observation and Geoinformation 30: 98–112.\n\n\nStanley, Janet, and Alan March. 2020. “Black Saturday: Urban Sprawl and Climate Change Remain Key Dangers.” Pursuit. https://pursuit.unimelb.edu.au/articles/black-saturday-urban-sprawl-and-climate-change-remain-key-dangers.\n\n\nVictoria State Government. 2017. “Plan Melbourne 2017–2050.” 2017. https://www.planning.vic.gov.au/guides-and-resources/strategies-and-initiatives/plan-melbourne.\n\n\nWilson, Katherine. 2024. “Guardian News & Media Produces a Variety of Content with Funding from Outside Parties.” The Guardian, May. https://www.theguardian.com/info/2017/may/22/guardian-news-media-funding-and-investment.\n\n\nYIMBY Melbourne. 2023. “Melbourne’s Missing Middle in 2023.” Melbourne, Australia: YIMBY Melbourne; Presentation."
  },
  {
    "objectID": "wk6.html#summary",
    "href": "wk6.html#summary",
    "title": "5  Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThe Google Earth Engine (GEE) is an amazing tool that allows for planetary scale analysis of remotely sensed data. This software enables easy access of satellite data from Landsat, Sentinel and many other data products directly in the platform, using the Javascript-based code editor. The extension of the tool to Python is particularly useful with the geemap Python package (Wu 2020). The data processing is done on Google’s servers, which combined with the readily accessible datasets makes GEE an immensely powerful research tool.\nAll the operations and objects are stored on the server, and interacting with them is done by writing specific code in JS that packaged into JSON is sent to the server to execute. Using Google’s servers allows for parallelisation of operations, which enables extremely large computational operations to complete in a reasonable time frame. That is why it is for example important to use map functions, rather than loops. GEE will also dynamically scale the resolution of satellite imagery, based on the request for optimal viewing."
  },
  {
    "objectID": "wk6.html#applications",
    "href": "wk6.html#applications",
    "title": "5  Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nUse of GEE in publications has skyrocketed since 2015, overtaking the number of newly published research papers where analysis was performed using Python, R or QGIS. A review of literature utilising GEE in methodology showed the potential of GEE for global analysis and research (Velastegui-Montoya et al. 2023). GEE has bee used in studies in disciplines ranging from earth and environmental science, social science, to engineering.\nWhile browsing the literature, several interesting papers caught my interest. Firstly, this study by Tripathy and Malladi (2022) used GEE to develop the Global Flood Mapper tool, which uses a combination of both ascending and descending SAR data accessible from within the platform to extract change from images before and during a flooding event. The methodology accounts for the standard deviation of pixel values in the collections of images, which enables the quantification of confidence level for flood extent mapping. The use of GEE enables the deployment of this methodology globally, making an accessible and powerful research tool, which I accessed to map the Melbourne floods from October 2022, shown below:\n\n\n\nMelbourne October 2022 floods in Global Flood Mapper. Source: Global Flood Mapper by Tripathy and Malladi (2022)\n\n\nAnother paper by Liu et al. (2018) used GEE to map global urbanisation between 1990 and 2010, showing urban land has increased by around 43% in these years. The authors estimate it using a custom Normalized Urban Areas Composite Index from Landsat 5 data, constructed from several established indices, including NDVI, and make the dataset from this analysis publicly available. Another study used the MODIS satellite data for wildfire damage assessment for all of Australia, based on land cover classification of pre and post-fire images (Seydi et al. 2021)."
  },
  {
    "objectID": "wk6.html#reflections",
    "href": "wk6.html#reflections",
    "title": "5  Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nIn the practical sessions I got to use GEE for the first time, which involved loading some Landsat data and running certain operations. Playing around with the data in GEE, I certainly realised what potential it has for large-scale analysis. I added some satellite data for Melbourne and understood in practice how mosaicking is applied to multiple overlapping images, as well as applied some texture measures and PCA to the image.\n\n\n\nExploring Google Earth Engine for the first time! (of course in Melbourne)\n\n\nOverall, it seems to me that the power of GEE for global research cannot be overestimated, as it enables truly impressive large-scale analysis to be run by anyone with internet access, for free. I previously did not realise how powerful this tool is and I cannot wait to see where this course will take me further in regards to GEE. As a sidenote, I am not a huge fan of the GUI of GEE - Google could probably do much better and I hope they do.\n\n\n\n\nLiu, Xiaoping, Guohua Hu, Yimin Chen, Xia Li, Xiaocong Xu, Shaoying Li, Fengsong Pei, and Shaojian Wang. 2018. “High-Resolution Multi-Temporal Mapping of Global Urban Land Using Landsat Images Based on the Google Earth Engine Platform.” Remote Sensing of Environment 209: 227–39.\n\n\nSeydi, Seyd Teymoor, Mehdi Akhoondzadeh, Meisam Amani, and Sahel Mahdavi. 2021. “Wildfire Damage Assessment over Australia Using Sentinel-2 Imagery and MODIS Land Cover Product Within the Google Earth Engine Cloud Platform.” Remote Sensing 13 (2): 220.\n\n\nTripathy, Pratyush, and Teja Malladi. 2022. “Global Flood Mapper: A Novel Google Earth Engine Application for Rapid Flood Mapping Using Sentinel-1 SAR.” Natural Hazards 114 (2): 1341–63.\n\n\nVelastegui-Montoya, Andrés, Néstor Montalván-Burbano, Paúl Carrión-Mero, Hugo Rivera-Torres, Luı́s Sadeck, and Marcos Adami. 2023. “Google Earth Engine: A Global Analysis and Future Trends.” Remote Sensing 15 (14): 3675.\n\n\nWu, Qiusheng. 2020. “Geemap: A Python Package for Interactive Mapping with Google Earth Engine.” Journal of Open Source Software 5 (51): 2305."
  },
  {
    "objectID": "wk7.html#summary",
    "href": "wk7.html#summary",
    "title": "6  Classification",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nThis week I learned more about applying machine learning methods to raster data. We looked specifically at two types of analysis - classification and regression. Such models utilise an expert system, which is a mechanism that draws conclusions from a knowledge base it was exposed to during training. Machine learning on raster images can be broadly divided into object-based and pixel-based analysis.\nDecision Trees - Classification\nThis is an algorithm for splitting up the dataset into classes based on values of predictors. Decision trees are built up of decision nodes where data points are split, and end-nodes (leaves), which match the data that falls into that leaf to a particular class. Basically it’s classifying the data based on many if else statements. However, the mechanism for constructing such trees can be more complex.\nIn order to decide how best to split the data, several measures can be used, such that the decision nodes split the data in the most effective way. Methods include entropy, information gain, or the most commonly used - Gini impurity, which is calculated for every decision node, and takes the form of the equation below:\n\\[\nginiImpurity(D) = 1 - \\sum_{i=1}^{k}{p_i^2}\n\\]\nwhere \\(p\\) is the probability of datapoints belonging to class \\(i\\) at node \\(D\\), and \\(k\\) is the number of classes. When building the classification tree, the nodes with lowest Gini impurity are selected. The trees are evaluated iteratively, fitting lines to all groups and keeping nodes with lowest sum of squared residuals (SSR).\nThe problem of overfitting decision trees - when they learn too granular aspects of the data and cannot generalise to the unseen data can be combated in several ways. The most common method is pruning based on the weakest link. This means removing decision nodes, which are the least important in classifying training data into the correct classes. This is based on the tree score, which can be calculated using the function below:\n\\[\ntreeScore(Tree) = SSR + \\alpha T\n\\] where \\(\\alpha\\) is the tree penalty parameter and \\(T\\) is the number of leaves. The process works by iteratively incrementing alpha. Therefore, pruning is used to build the tree, aiming to optimise the number of decisions in order to make the trees parsimonious. Cross validation can be used to find the best alpha.\nRegression Trees\nThe same methodology of decision trees extends to regression problems, in which we need to predict a number rather than a class. The tree is built by splitting the dataset into groups along one of the axes, and a decision node is added at the point where the SSR resulting from that division is lowest. The first decision node is chosen for the predictor whose split results in the lowest SSR, and the process is repeated iteratively until the number of splits reaches the limit imposed by the modeller, such as minimum number of data points for a leaf.\n\n\n\nIllustration of a Regression Tree applied to a dataset. Source: datacamp.com\n\n\nRandom forest\nThis method generalises the above by building many random decision trees. We have little control of how individual trees are built. However, many simple trees overall vote for the final prediction, which works very well for many problems. The training of random forests benefits from bootstrapping, which is a method that randomly subsets the training data, such that every tree is trained on a different subset. An out-of-bag hold out set of the data can be used to evaluate the trained model.\nSupport Vector Machines\nSVMs are a method that finds a hyperplane in the multidimensional feature space, which best divides that data into classes. The method can benefit from applying kernel functions, which project the data points onto hyperplanes, which are easier to classify. The hyperparameters, which control the model can be tuned, e.g. using grid search, which tests many predefined values of parameters (very expensive), or using an optimisation algorithm, such as simulated annealing, swarm optimisation, or genetic algorithms.\nUnsupervised methods\nAdditionally, unsupervised methods can also be used for identifying unspecified classes. Algorithms such as DBSCAN or k-means clustering, will assign data points to optimal classes, based on the patterns within it. When a cluster is identified that comprises two known classes, cluster busting can be performed, which is reapplying a clustering algorithm to that one cluster.\nApplying these methods to satellite imagery can be done for every pixel, where e.g. a land cover class is assigned to every pixel (classification) or an index value (regression), and features are channels."
  },
  {
    "objectID": "wk7.html#applications",
    "href": "wk7.html#applications",
    "title": "6  Classification",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nThere are many examples of research papers applying machine learning methods to remotely sensed data in urban research. The first one I wanted to focus on here is a paper that takes a two-pronged fuzzy approach to classification, in that land cover classes are assigned probabilities to pixels, rather than definitive classes (Shackelford and Davis 2003). In the model, similar classes such as roads and buildings would in such a case be most difficult to classify, especially since pixel-based analysis doesn’t consider spatial dependence. After performing pixel-based classification, the authors develop a second method, that performs object-based segmentation of the image based on similarity of classes in nearby pixels, which makes the output more consistent, resulting in an overall more flexible methodology.\n\n\n\nOutput of fuzzy classifier for dense urban image (Shackelford and Davis 2003)\n\n\nAnother paper by Goldblatt et al. (2016) is aimed at identifying boundaries of urban areas using a pixel based analysis of remotely sensed data, using SVM and Random Forest models. The model outputs of pixel-based classification had to be post-processed to remove single pixels classified as urban, which smooths the final classification. Furthermore, the authors develop the model in GEE, which enables the research to be easily replicated for other countries, which poses opportunities for advancing future research."
  },
  {
    "objectID": "wk7.html#reflections",
    "href": "wk7.html#reflections",
    "title": "6  Classification",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nThis week was actually quite unique as it offered a perspective on working with image data I had not considered before. Prior to this lecture, I had only learned about object-based analysis of images with convolutional operations, and specifically using neural networks to extract and encode information from images in order to classify or identify features from images. This lecture opened my eyes to much simpler way of working with satellite imagery that uses only pixel values of various bands (features) to predict such outcomes.\nThis enables the use of classical machine learning algorithms like e.g. Random Forests in the context of raster data, by treating is just like tabular data. This gets rid of the spatial dependence of pixels that are next to each other, which of course is a significant limitation, but such methods often can achieve the same performance at a significantly computationally cheaper cost than convolutional neural networks.\n\n\n\n\nGoldblatt, Ran, Wei You, Gordon Hanson, and Amit K Khandelwal. 2016. “Detecting the Boundaries of Urban Areas in India: A Dataset for Pixel-Based Image Classification in Google Earth Engine.” Remote Sensing 8 (8): 634.\n\n\nShackelford, Aaron K, and Curt H Davis. 2003. “A Combined Fuzzy Pixel-Based and Object-Based Approach for Classification of High-Resolution Multispectral Data over Urban Areas.” IEEE Transactions on GeoScience and Remote Sensing 41 (10): 2354–63."
  },
  {
    "objectID": "wk4.html#references",
    "href": "wk4.html#references",
    "title": "4  Policy applications",
    "section": "4.4 References",
    "text": "4.4 References\n\n\n\n\nHazeu, Gerard, Pavel Milenov, Bas Pedroli, Vessela Samoungi, Michiel Van Eupen, and Vassil Vassilev. 2014. “High Nature Value Farmland Identification from Satellite Imagery, a Comparison of Two Methodological Approaches.” International Journal of Applied Earth Observation and Geoinformation 30: 98–112.\n\n\nStanley, Janet, and Alan March. 2020. “Black Saturday: Urban Sprawl and Climate Change Remain Key Dangers.” Pursuit. https://pursuit.unimelb.edu.au/articles/black-saturday-urban-sprawl-and-climate-change-remain-key-dangers.\n\n\nVictoria State Government. 2017. “Plan Melbourne 2017–2050.” 2017. https://www.planning.vic.gov.au/guides-and-resources/strategies-and-initiatives/plan-melbourne.\n\n\nWilson, Katherine. 2024. “Guardian News & Media Produces a Variety of Content with Funding from Outside Parties.” The Guardian, May. https://www.theguardian.com/info/2017/may/22/guardian-news-media-funding-and-investment.\n\n\nYIMBY Melbourne. 2023. “Melbourne’s Missing Middle in 2023.” Melbourne, Australia: YIMBY Melbourne; Presentation."
  },
  {
    "objectID": "wk9.html#summary",
    "href": "wk9.html#summary",
    "title": "8  Synthetic Aperture Radar",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nSynthetic Aperture Radar (SAR) is an active type of sensor that sends a signal at a specific frequency to Earth’s surface and records the signal in that same frequency coming back. The choice of the frequency of course will determine what the sensor is able to identify. For example sea band radio waves (4-8 GHz) can be used for detecting ice or ocean maritime activity. This is why depending on the wavelength we are able to penetrate e.g. through the tree canopy, as well as clouds. Another advantage is that this can collect data at night, as sun’s energy is not utilised.\nAperture of the radar is of significance, as that determines the resolution of the collected data. This is controlled by the size of the antenna, and the longer the antenna the higher resolution we could collect. For that reason, this sensor utilises the movement of the satellite in orbit to “expand” the antenna, because it is continuously sending signal to Earth imaging the specific location from different points in orbit. Hence the name - synthetic aperture radar. The image below illustrates how this works.\n\n\n\nIllustration of SAR. Source: DLR (CC BY-NC-ND 3.0)\n\n\nAnother notable aspect of SAR imagery is the polarisation of the signal. Different sensors will have different polarisation (either vertical or horizontal), which affects what materials are recorded. For example buildings and trees are most sensitive to sensor sending and receiving horizontal polarisations, while bare earth to vertical-vertical (VV). Therefore, for example it is of significance to capturing water’s surface, whether the imagery is collected on a windy or calm day, as rough water surface will reflect signal differently.\nBackscatter or amplitude of the signal, as well as its phase are also of significance. Phase is when in the wave cycle the signal reaches the sensor (whether its at a peak or trough of the wave). This is the technique, which enables change detection with very high accuracy. This is called Interferometric SAR (InSAR), which can be used for creating Digital Elevation Models (DEMs). Furthermore, Differential InSAR (DInSAR) can also be used for detecting change after earthquakes, landslides or other events that had an impact on the earth’s surface, but also buildings after for example bombings.\nSAR imagery when captured, of course also comes with lots of metadata, which is crucial to understand and analyse the imagery, for example it will matter whether the data was recorded on the south-to-north or north-to-south orbit pass. Moreover, SAR data will often be on a power scale, which necessitates the use of appropriate transformation methods. In practice, perhaps the most influential use case of SAR data is for change detection, which is based on a ratio of the image after a change event to the image prior. There have been many ways developed for quantifying change, depending on the intended use case, such as inverted ratio which works in ways that gives changed pixels more importance."
  },
  {
    "objectID": "wk9.html#reflections",
    "href": "wk9.html#reflections",
    "title": "8  Synthetic Aperture Radar",
    "section": "8.3 Reflections",
    "text": "8.3 Reflections\nSAR seems like a great technology, which has a lot of applications in very important fields, like disaster response, thanks to its usability for change detection at very fine scales. I see very wide potential for the use of SAR data especially considering ability to penetrate through clouds or trees, depending on the frequency chosen. The consideration of VH vs VV data is very interesting to me, as I can imagine the data would often show quite different things, depending from which side the satellite is approaching and the location is sensed.\n\n\n\n\nKajimoto, Muneyoshi, and Junichi Susaki. 2013. “Urban Density Estimation from Polarimetric SAR Images Based on a POA Correction Method.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 6 (3): 1418–29.\n\n\nLiu, Wen, Masashi Matsuoka, Fumio Yamazaki, Takashi Nonaka, and Tadashi Sasagawa. 2013. “Detection of Surface Displacements and Liquefied Areas in the 2011 Christchurch Earthquake from SAR Data.”\n\n\nTripathy, Pratyush, and Teja Malladi. 2022. “Global Flood Mapper: A Novel Google Earth Engine Application for Rapid Flood Mapping Using Sentinel-1 SAR.” Natural Hazards 114 (2): 1341–63."
  },
  {
    "objectID": "wk8.html",
    "href": "wk8.html",
    "title": "7  Assessment of classification models",
    "section": "",
    "text": "8 Applications\nThe first interesting paper compares the performance of pixel-based vs object-based analysis using 3 different classification algorithms for agricultural land cover classification (Duro, Franklin, and Dubé 2012). The authors found a statistically significant difference is better performance of the Decision Tree algorithm, when using object-based analysis, while in pixel-based analysis the three models produced the same results. The conclusion of the paper states that although pixel-based analysis was computationally cheaper to run, there was no advantage over using either approach on this problem.\nThe lecture made me think about the regularity of the superpixel grid created by applying SLIC, such that the index information of column and rows is retained. This could be used for running convolutional filters on such data. A quick literature search revealed that such a method has been developed and applied to neural networks for semantic segmentation (Suzuki et al. 2018).\nA very interesting approach to assessing the accuracy of a classification model was proposed by Oort et al. (2004), who argue that the spatial variability of a model’s accuracy needs to be quantified. The approach uses a 3x3 window surrounding a pixel, and the local accuracy measure is calculated from the neighbouring pixels. The authors develop this method for land-cover classification in Netherlands and find that that local accuracy was higher in regions where land-covers were more homogeneous.\nThere are so many different accuracy assessment indicators, yet not one of them will ever give you the whole picture. Even if a particular indicator takes into account different error types, like the F1 score, the metric is still only a single number which doesn’t convey the whole picture.\nIt seems to me that for urban problems, using object-based analysis such as convolutional neural networks is more important when working with very high resolution imagery (sub-meter resolution), while it may be less important when we have low-resolution imagery like 10 meters, in which case pixel-based analysis may be more efficient. That’s because the features, which would really make a difference for identifying in an urban environment would likely be quite granular, and that’s when spatial aspect of images would be most important to consider. But of course, this depends on the particular problem.\nFurthermore, in object-based analysis it is interesting to me that using a bottom-up vs top-down approach for segmentation into supercells will be more or less efficient based on the type of data we are working with. For example if we are looking to segment very detailed structure in the image, it may be quicker to arrive at the desired segmentation if starting from joining pixels into supercells. Whereas if we are working with a very high resolution images and want to segment large objects, it may make sense to use the top-down approach.These kinds of reflections are what make this course interesting."
  },
  {
    "objectID": "wk8.html#summary",
    "href": "wk8.html#summary",
    "title": "7  Assessment of classification models",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis week the teaching continued on the topic of classification, focusing on aspects like object-based analysis and how we can assess classification results. Firstly, let’s look at object-based, as opposed to pixel-based analysis.\nObject-based analysis utilises supercells, which are created by joining pixels into objects, according to the similarity of nearby pixels, aiming to maximise the homogeneity of pixel values within those superpixels. The methodology could also be top-down, where the whole image is divided into smaller and smaller superpixels. This for example helps with the issue of spatial autocorrelation between training and test data. To create supercells an algorithm such as Simple Linear Iterative Clustering (SLIC) can be used, which iteratively joins pixels into objects based on similarity.\n\n\n\nSupercells. Source: Jakub Nowosad\n\n\nAnother type of analysis uses the opposite approach - sub-pixel analysis. This is relevant when a pixel could be considered as having several classes, e.g. lying at the intersection of more than one class. In such cases we can use Spectral Mixture Analysis, which estimates the proportion of different classes per pixel. This is related to probabilistic classification, where a pixel may be assigned a probability of belonging to the given class.\nAccuracy Assessment\nClassification accuracy assessment can be measured with several metrics. Firstly, there are the producer accuracy (recall), which is calculated as the true positive \\(TP\\) rate over \\((TP + FP)\\), and the user’s accuracy (precision), measuring the rate of pixels being misclassified per specific class - \\((TP)/(TP+FP)\\). The overall accuracy can be calculated from these.\nThe confusion matrix is a very useful tool that can often be created for a classification problem. It shows how wrong the model is through TP, TN, FP and FN rates.\nAnother measure of accuracy of classification is the kappa coefficient, which however is not regarded as an appropriate metric, due to several limitations. It is defined as\n\\[\n{\\displaystyle \\kappa ={\\frac {2\\times (TP\\times TN-FN\\times FP)}{(TP+FP)\\times (FP+TN)+(TP+FN)\\times (FN+TN)}}}\n\\]\nAnother metric is the F1 score, which however does not account for \\(TN\\). This metric aims to account for the issue of class imbalance, as it considers the relative performance. It is defined:\n\\[F1 = \\frac{2 \\times \\text{TP}}{2 \\times \\text{TP} + \\text{FP} + \\text{FN}}\\]\nLastly, the Receiver Operating Characteristic (ROC) Curve, which shows the accuracy of predictions using the true positive rate and false positive rate in one plot. Area under the curve gives a quantifiable measure for this method.\n\n\n\nROC illustration. Source: Analytics Vidhya\n\n\nAnother important consideration for model training is the train-test split, which is a fundamental step before training any machine learning model. It is because, only evaluating the model on unseen data can give an accurate assessment of its predictive power. Cross validation is a method that repeats this process, which involves training a model multiple times on different partitions of the dataset and evaluating it on the testing set.\nTo account for spatial autocorrelation of testing and training data, the use of spatial cross validation is recommended, which ensures that points from different locations are used for training and testing the model at each time, limiting the risk of data leakage due to spatial dependence of testing and training data (Karasiak et al. 2022). Furthermore, spatial partitioning can be used to distinguish regions of the study area, which often can be hierarchical."
  },
  {
    "objectID": "wk8.html#reflections",
    "href": "wk8.html#reflections",
    "title": "7  Assessment of classification models",
    "section": "7.3 Reflections",
    "text": "7.3 Reflections\nThere are so many different accuracy assessment indicators, yet not one of them will ever give you the whole picture. Even if a particular indicator takes into account different error types, like the F1 score, the metric is still only a single number which doesn’t convey the whole picture.\nIt seems to me that for urban problems, using object-based analysis such as convolutional neural networks is more important when working with very high resolution imagery (sub-meter resolution), while it may be less important when we have low-resolution imagery like 10 meters, in which case pixel-based analysis may be more efficient. That’s because the features, which would really make a difference for identifying in an urban environment would likely be quite granular, and that’s when spatial aspect of images would be most important to consider. But of course, this depends on the particular problem.\nFurthermore, in object-based analysis it is interesting to me that using a bottom-up vs top-down approach for segmentation into supercells will be more or less efficient based on the type of data we are working with. For example if we are looking to segment very detailed structure in the image, it may be quicker to arrive at the desired segmentation if starting from joining pixels into supercells. Whereas if we are working with a very high resolution images and want to segment large objects, it may make sense to use the top-down approach.These kinds of reflections are what make this course interesting.\n\n\n\n\nDuro, Dennis C, Steven E Franklin, and Monique G Dubé. 2012. “A Comparison of Pixel-Based and Object-Based Image Analysis with Selected Machine Learning Algorithms for the Classification of Agricultural Landscapes Using SPOT-5 HRG Imagery.” Remote Sensing of Environment 118: 259–72.\n\n\nKarasiak, Nicolas, J-F Dejoux, Claude Monteil, and David Sheeren. 2022. “Spatial Dependence Between Training and Test Sets: Another Pitfall of Classification Accuracy Assessment in Remote Sensing.” Machine Learning 111 (7): 2715–40.\n\n\nOort, Pepijn AJ van, Arnold K Bregt, Sytze de Bruin, Allard JW de Wit, and Alfred Stein. 2004. “Spatial Variability in Classification Accuracy of Agricultural Crops in the Dutch National Land-Cover Database.” International Journal of Geographical Information Science 18 (6): 611–26.\n\n\nSuzuki, Teppei, Shuichi Akizuki, Naoki Kato, and Yoshimitsu Aoki. 2018. “Superpixel Convolution for Segmentation.” In 2018 25th IEEE International Conference on Image Processing (ICIP), 3249–53. IEEE."
  },
  {
    "objectID": "wk9.html",
    "href": "wk9.html",
    "title": "8  Synthetic Aperture Radar",
    "section": "",
    "text": "9 Applications\nThe first study I wanted to describe, used SAR data for detecting fault slip change after the 2011 earthquake in Christchurch, New Zealand (Liu et al. 2013). The authors utilise data from ALOS/PALSAR and TerraSAR-X (TSX) of the area before and after the earthquake, and use InSAR for detecting ground movements and liquefied areas. The figures below show their results, from which it can be clearly seen that the city center of Christchurch was the most affected.\nAnother paper used SAR data for estimating urban density of buildings within urban environments (Kajimoto and Susaki 2013). The main issue that the authors address with their methodology arises from the backscattering of signal from buildings in densely built-up urban areas. The proposed methodology addresses the issue by adjusting the measurements to account for the polarisation orientation angle and normalising the data.\nSAR can also been used for flood detection by extracting the difference between two images of before and during a flooding event (Tripathy and Malladi 2022). However, the authors discuss a crucial issue - there is a lack of consensus for whether to use ascending or descending orbit data for flood detection, as often it can depend on local wind direction. That’s why the authors develop a method to quantify the difference in images using both VV and VH data. Their methodology also allows for quantifying uncertainty of a flood by calculating the standard deviation of pixel values from the pre-flooding image collection."
  },
  {
    "objectID": "wk8.html#applications",
    "href": "wk8.html#applications",
    "title": "7  Assessment of classification models",
    "section": "7.2 Applications",
    "text": "7.2 Applications\nThe first interesting paper compares the performance of pixel-based vs object-based analysis using 3 different classification algorithms for agricultural land cover classification (Duro, Franklin, and Dubé 2012). The authors found a statistically significant difference is better performance of the Decision Tree algorithm, when using object-based analysis, while in pixel-based analysis the three models produced the same results. The conclusion of the paper states that although pixel-based analysis was computationally cheaper to run, there was no advantage over using either approach on this problem.\nThe lecture made me think about the regularity of the superpixel grid created by applying SLIC, such that the index information of column and rows is retained. This could be used for running convolutional filters on such data. A quick literature search revealed that such a method has been developed and applied to neural networks for semantic segmentation (Suzuki et al. 2018).\n\n\n\nIllustration of convolution operation applied to superpixels. Source: Suzuki et al. (2018)\n\n\nA very interesting approach to assessing the accuracy of a classification model was proposed by Oort et al. (2004), who argue that the spatial variability of a model’s accuracy needs to be quantified. The approach uses a 3x3 window surrounding a pixel, and the local accuracy measure is calculated from the neighbouring pixels. The authors develop this method for land-cover classification in Netherlands and find that that local accuracy was higher in regions where land-covers were more homogeneous."
  },
  {
    "objectID": "wk9.html#applications",
    "href": "wk9.html#applications",
    "title": "8  Synthetic Aperture Radar",
    "section": "8.2 Applications",
    "text": "8.2 Applications\nThe first study I wanted to describe, used SAR data for detecting fault slip change after the 2011 earthquake in Christchurch, New Zealand (Liu et al. 2013). The authors utilise data from ALOS/PALSAR and TerraSAR-X (TSX) of the area before and after the earthquake, and use InSAR for detecting ground movements and liquefied areas. The figures below show their results, from which it can be clearly seen that the city center of Christchurch was the most affected.\n\n\n\nInSAR image showing the effects of the 2011 earthquake in Christchurch. Source: Liu W. et al. 2013\n\n\nAnother paper used SAR data for estimating urban density of buildings within urban environments (Kajimoto and Susaki 2013). The main issue that the authors address with their methodology arises from the backscattering of signal from buildings in densely built-up urban areas. The proposed methodology addresses the issue by adjusting the measurements to account for the polarisation orientation angle and normalising the data.\nSAR can also been used for flood detection by extracting the difference between two images of before and during a flooding event (Tripathy and Malladi 2022). However, the authors discuss a crucial issue - there is a lack of consensus for whether to use ascending or descending orbit data for flood detection, as often it can depend on local wind direction. That’s why the authors develop a method to quantify the difference in images using both VV and VH data. Their methodology also allows for quantifying uncertainty of a flood by calculating the standard deviation of pixel values from the pre-flooding image collection."
  }
]