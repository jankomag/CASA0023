[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CASA0023",
    "section": "",
    "text": "Preface\n\nCASA0023 Learning Diary\nThis is the learning diary created as part of the module CASA0023 Remotely Sensing Cities and Environments. Here, I will be adding a summary of the lecture contents, personal reflections and some discussion about academic literature relevant to each week’s topic."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "wk1.html",
    "href": "wk1.html",
    "title": "1  Week 1",
    "section": "",
    "text": "This week we were introduced to remote sensing, focusing on how satellite imagery is collected. Firstly, there are two types of sources, passive and active - which use their own energy source to monitor Earth (e.g. SAR). The signal is recorded by capturing electromagnetic waves, which can have different frequencies, enabling sensors to find collect information about things that are not visible to human eye, because different objects have different spectral signature."
  },
  {
    "objectID": "wk2.html",
    "href": "wk2.html",
    "title": "2  Week 2 - Xaringan Presentation",
    "section": "",
    "text": "Here is the presentation on Landsat 9:"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "::: {#refs}\n\n\n\n\n\nGoldblatt, Ran, Wei You, Gordon Hanson, and Amit K Khandelwal. 2016.\n“Detecting the Boundaries of Urban Areas in India: A Dataset for\nPixel-Based Image Classification in Google Earth Engine.”\nRemote Sensing 8 (8): 634.\n\n\nHazeu, Gerard, Pavel Milenov, Bas Pedroli, Vessela Samoungi, Michiel Van\nEupen, and Vassil Vassilev. 2014. “High Nature Value Farmland\nIdentification from Satellite Imagery, a Comparison of Two\nMethodological Approaches.” International Journal of Applied\nEarth Observation and Geoinformation 30: 98–112.\n\n\nKit, Oleksandr, Matthias Lüdeke, and Diana Reckien. 2012.\n“Texture-Based Identification of Urban Slums in Hyderabad, India\nUsing Remote Sensing Data.” Applied Geography 32 (2):\n660–67.\n\n\nLaw, W Boone, Peter Hiscock, Bertram Ostendorf, and Megan Lewis. 2021.\n“Using Satellite Imagery to Evaluate Precontact Aboriginal\nForaging Habitats in the Australian Western Desert.”\nScientific Reports 11 (1): 10755.\n\n\nLi, Wei, Jean-Daniel M Saphores, and Thomas W Gillespie. 2015. “A\nComparison of the Economic Benefits of Urban Green Spaces Estimated with\nNDVI and with High-Resolution Land Cover Data.” Landscape and\nUrban Planning 133: 105–17.\n\n\nLiu, Xiaoping, Guohua Hu, Yimin Chen, Xia Li, Xiaocong Xu, Shaoying Li,\nFengsong Pei, and Shaojian Wang. 2018. “High-Resolution\nMulti-Temporal Mapping of Global Urban Land Using Landsat Images Based\non the Google Earth Engine Platform.” Remote Sensing of\nEnvironment 209: 227–39.\n\n\nNazeer, Majid, Janet E Nichol, and Ying-Kit Yung. 2014.\n“Evaluation of Atmospheric Correction Models and Landsat Surface\nReflectance Product in an Urban Coastal Environment.”\nInternational Journal of Remote Sensing 35 (16): 6271–91.\n\n\nSeydi, Seyd Teymoor, Mehdi Akhoondzadeh, Meisam Amani, and Sahel\nMahdavi. 2021. “Wildfire Damage Assessment over Australia Using\nSentinel-2 Imagery and MODIS Land Cover Product Within the Google Earth\nEngine Cloud Platform.” Remote Sensing 13 (2): 220.\n\n\nShackelford, Aaron K, and Curt H Davis. 2003. “A Combined Fuzzy\nPixel-Based and Object-Based Approach for Classification of\nHigh-Resolution Multispectral Data over Urban Areas.” IEEE\nTransactions on GeoScience and Remote Sensing 41 (10): 2354–63.\n\n\nStanley, Janet, and Alan March. 2020. “Black Saturday: Urban\nSprawl and Climate Change Remain Key Dangers.” Pursuit.\nhttps://pursuit.unimelb.edu.au/articles/black-saturday-urban-sprawl-and-climate-change-remain-key-dangers.\n\n\nTaherzadeh, Ebrahim, and Helmi ZM Shafri. 2011. “Using\nHyperspectral Remote Sensing Data in Urban Mapping over Kuala\nLumpur.” In 2011 Joint Urban Remote Sensing Event,\n405–8. IEEE.\n\n\nTuia, Devis, Frédéric Ratle, Alexei Pozdnoukhov, and Gustavo\nCamps-Valls. 2009. “Multisource Composite Kernels for Urban-Image\nClassification.” IEEE Geoscience and Remote Sensing\nLetters 7 (1): 88–92.\n\n\nVelastegui-Montoya, Andrés, Néstor Montalván-Burbano, Paúl Carrión-Mero,\nHugo Rivera-Torres, Luı́s Sadeck, and Marcos Adami. 2023. “Google\nEarth Engine: A Global Analysis and Future Trends.” Remote\nSensing 15 (14): 3675.\n\n\nVictoria State Government. 2017. “Plan Melbourne\n2017–2050.” 2017. https://www.planning.vic.gov.au/guides-and-resources/strategies-and-initiatives/plan-melbourne.\n\n\nWilson, Katherine. 2024. “Guardian News & Media Produces a\nVariety of Content with Funding from Outside Parties.” The\nGuardian, May. https://www.theguardian.com/info/2017/may/22/guardian-news-media-funding-and-investment."
  },
  {
    "objectID": "index.html#casa0023-learning-diary",
    "href": "index.html#casa0023-learning-diary",
    "title": "CASA0023",
    "section": "CASA0023 Learning Diary",
    "text": "CASA0023 Learning Diary\nThis is the Portfolio for CASA0023 Remotely Sensing Cities and Environments, created with Quarto."
  },
  {
    "objectID": "wk1.html#introduction-to-remote-sensing",
    "href": "wk1.html#introduction-to-remote-sensing",
    "title": "1  Week 1",
    "section": "1.1 Introduction to Remote Sensing",
    "text": "1.1 Introduction to Remote Sensing\nIn the first week of the module we were introduced to remote sensing, focusing on the principles of what satellite imagery is and how it’s collected. The first classification of sensor types, is into passive and active sources, the former use the sun’s energy reflected off the ground, while the latter use their own energy source to send signal to Earth and record it (e.g. SAR). The sensors record the signal by capturing electromagnetic waves, which can have different frequencies, enabling sensors to collect information about things that are not visible to human eye, due to varying spectral signatures of different objects. The figure below shows such a spectrum of wavelengths that signal can take, showing a rather small part that is actually visible.\n\n\n\nElectromagnetic spectrum. Image source: SkyWatch.com\n\n\nThere are certain important considerations that we need to be aware of about the signal that is being collected by the satellites. Firstly, as the signal travels, it passes through the atmosphere, which can interrupt it causing atmospheric haze of various types. Furthermore, because smaller wavelengths scatter easier, they are more difficult to pick up by the sensor.\n\n1.1.1 The Four Resolutions\nThere are four resolutions of images captured by satellites. Firstly, spatial - which tells us how spatially precise an image is, e.g. 10m per px. Spectral resolution refers to the bands which have been recorded, as not all wavelengths are observable; images with many bands are referred to as multispectral or hyperspectral. Radiometric resolution describes the sensitivity e.g. 8-bit; and finally the temporal resolution is how often the certain place on Earth is photographed.\n\n\n1.1.2 Merging bands\nIndividual bands can be merged to create indices, which show specific characteristics of the Earth’s surface we want to measure. In a situation where bands are not in the same spatial resolutions, images need to be resampled into the same resolution, in order to be able to work with such data conjointly. There are two options for this: downscaling - going from a higher resolution like 10m to a lower like 30m; or upscaling, which is the opposite operation. The former is preferred in most cases, as the transformation makes fewer assumptions. An example of an index that can be computed by merging specific bands in a certain way is the tasseled cap transformation, which is calculated from the bands representing brightness, greenness and wetness. The resulting index can be used to identify vegetation, as well as urban areas.\n\n\n1.1.3 Working with sattelite imagery data\nSatellite data taken by Sentinel satellites can be freely accessed from the Copernicus Open Access Hub. Software tools like QGIS and SNAP (specifically for Sentinel data) can be used to browse and analyse this data, e.g. for combining different bands to produce desired indices. Examples of such indices include false colour composite, or atmospheric penetration composite, which allow us to visualise features of the environment that are not visible to the human eye. Landsat images, which are free global images of temporal resolution 16 days, can also be acquired from the USGS website."
  },
  {
    "objectID": "wk1.html#practcial-examples",
    "href": "wk1.html#practcial-examples",
    "title": "1  Week 1",
    "section": "1.2 Practcial examples",
    "text": "1.2 Practcial examples\nFor the practical part of this week, I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\nThese figures show this reflectance, e.g. band 5 produces the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels.\n\n\n\nSpectralReflectance"
  },
  {
    "objectID": "wk3.html#landsat-data",
    "href": "wk3.html#landsat-data",
    "title": "3  Week 3",
    "section": "3.1 Landsat Data",
    "text": "3.1 Landsat Data\nComposed of different satellites; Landsat 1-9\nOriginal Multispectral Camera (MSS) proposed by Virginia Norwood, wiht innovative design, whisk broom, not push broom. Pivoting mirror not scanner. NASA doubted digital camera originally, wanted RBV, limited to b, g, r, near-infrred."
  },
  {
    "objectID": "wk3.html#two-types-of-sensors",
    "href": "wk3.html#two-types-of-sensors",
    "title": "3  Week 3",
    "section": "3.2 Two types of sensors:",
    "text": "3.2 Two types of sensors:\n\nRBV:\nMSS:"
  },
  {
    "objectID": "wk3.html#data",
    "href": "wk3.html#data",
    "title": "3  Week 3",
    "section": "3.2 Data",
    "text": "3.2 Data\nData was made publicly available in 2008"
  },
  {
    "objectID": "wk3.html#corrections",
    "href": "wk3.html#corrections",
    "title": "3  Week 3",
    "section": "3.1 Corrections",
    "text": "3.1 Corrections\nThere are several issues with raw satellites imagery that require their corrections before using in analysis. Nowadays, the images are already corrected in most cases, but it’s good to understand what types of corrections need to be applied, and why. One example is a correction due to scan lines - gaps between images, which need to be smoothed.\n\n3.1.1 Geometric correction\nThis is a correction that is needed for images taken at off-nadir angle, meaning they are not looking directly down. We can imagine that the topography of the land, especially if there are mountains would significantly impact what the image shows when taken at an angle. The correction is done by mapping the points on a pre-correction image to the same points in the ground truth image. The correction can be done with OLS regression between the point values from the gold standard and the distorted image, where we’re looking to minimise the RMSE. There are two methods for this - input to output (forwrad mapping) and output to input (backward mapping). The latter is usually the default, and is done by mapping pixels from the distorted image pixel onto the gold standard image. This correction is basically like georeferencing an image, which does not have coordinates.\n\n\n3.1.2 Atmospheric correction\nThis correction is needed when the illumination source - sun going though the atmosphere is distorted resulting in haze and scattering in the images. This shows up in images as the adjacency effect, where pixel values switch their position. One method to deal with it is dark object subtraction, where the value of the darkest pixel is subtracted from the whole image. Second option is the pseudo-invariant feature.\nThere are three main types for this corrcetion: - absolute correction, where the atmposhpere is modelled with atmospheric radiative transfer models and controlled for/\n\nempirical correction type, that involves taking measurements on earth, which are used as ground truth to fit a linear regression model to minimise distortion.\n\nAlso worth mentioning is that the path radiance can be disturbed in different ways. Firstly radiance can get reflected in the atmosphere, scattering above the surface. Secondly, the light can get absorbed by the particles in the atmosphere, which results in not reahing the sensor, this is atmospheric attenuation.\nThe correction for this can be done first on several points in order to estimate the coefficients, which are then used to apply the correction apply to the rest of the image.\n\n\n3.1.3 Orthorectification / topographic correction\nThis is used to force pixels to be viewed at nadir angle. To correct for this, we can use a cosine correction, which requires information on the sensor geometry and an elevation model of the area imaged.\n\n\n3.1.4 Radiometric Calibration\nThis is not a correction but a calibration that converts the raw pixel values of an image from digital numbers to reflectance. In raw images, the pixels have image brightness or radiance recorded as a Digital Number (DN), which represents the intensity of the electromagnetic radiation per pixel. The difference between radiance and reflectance is that the former is amount of light recorded by the sensor, and the latter is the ratio of signal reflected from an object, to the amount recorded by the sensor.\nThus, radiometric Calibration is converting DN to spectral radiance, calculated using values that the sensor was calibrated for, before deployed on satellites.\nIn practice, most of the data we use, comes already analysis-ready. In Landsat algorithms like LEDPAS or L8SR apply the relevant corrections, and they are the Level 2 product."
  },
  {
    "objectID": "wk3.html#enhancements-and-data-joining",
    "href": "wk3.html#enhancements-and-data-joining",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.2 Enhancements and data joining",
    "text": "3.2 Enhancements and data joining\nThis section describes some manipulations that can be done on the corrected images. Firstly, we may need to join datasets as images naturally come in tiles. The edges of such tiles will usually overlap, in which case we can blend images into each other by mosaicking, or feathering.\n\n3.2.1 Filters\nApplying filters to an image is like applying a convolution kernel to an image. Different filters can be used to enhance, extract or smooth certain features of the image.\n####Texture Various operations can be done with filters, e.g. mean, variance, entropy and other operations are done by calculating probability, by dividing the values by the number of pixels in the kernel. The value of every pixel will then reflect, e.g. the variance of the kernel centered on that pixel.\nApplying the filters on the 4th band of the image of Dunedin from previous weeks, with a 7x7 window produced the measures shown here:\n Some of the measures may be difficult to interpret, as this was run on the band 4 representing infra-red - e.g. water is only distinctively shown in the mean texture, while high entropy is for example along the coastline.\nAnalysis will be updated.\nWe can also stack images on top of each other, e.g. from different dates, which is data fusion.\n\n3.2.1.1 PCA\nJust how PCA, a dimensionality reduction method can be used on tabular data, where the number of variables is reduced, so is it the same for satellite imagery. Linear combinations of multiple bands are combined in a way that maximises the variation in pixel values.\nAs part of the practical this method was applied on the Landsat data of Dunedin from previous weeks. The resulting principal components are presented in the figure below:\n\n\n\nPCA"
  },
  {
    "objectID": "wk1.html#practical-examples",
    "href": "wk1.html#practical-examples",
    "title": "1  Week 1",
    "section": "1.2 Practical examples",
    "text": "1.2 Practical examples\nFor the practical part of this week, I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\nThese figures show this reflectance, e.g. band 5 has the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels. These values, of course depend on the polygons that were selected.\n\n\n\nSpectralReflectance"
  },
  {
    "objectID": "wk1.html#applications",
    "href": "wk1.html#applications",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nThere are of course countless examples of research that uses remote sensing data. While browsing the literature I came across a project focused on using such data from different sources including Landsat to model the historic Aboriginal foraging habitats in the Australian Western Desert. This was done by assessing water accessibility, vegetation greenness, as well as the land topography, identifying locations where precontact Aboriginal people likely lived (Law et al. 2021). I find projects like this fascinating, as they show that remote sensing data enables us to study topics which would otherwise be impossible or very difficult to understand.\nStudies highlight the difficulties and the importance of certain consideration when working with data from satellites. For example, (Li, Saphores, and Gillespie 2015) use remote sensing data at two spatial resolutions - 30m of NDVI index and 0.6m for land cover classification, in order to investigate the economic benefits of urban green space. They show that although the NDVI data is more relevant, the higher resolution land cover is able to more precisely characterise this relationship, which shows the importance of high resolution data, especially when working at a city-scale.\nThe consideration of spectral resolutions for research likewise is very relevant. By using hyperspectral images at high spatial resolution (Taherzadeh and Shafri 2011) have been able to develop a model for classifying urban materials in Kuala Lumpur. The authors used a Support Vector Machine for this, showing its different predictive ability for different urban materials. The study also then used a Lee filter method to smooth the image, which improved the model’s predictions."
  },
  {
    "objectID": "wk1.html#reflections",
    "href": "wk1.html#reflections",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nThe practical part of this week was focused on getting started with downloading satellite imagery, both Landsat and Sentinel, and familiarising ourselves with the software, which can be used to analyse it. I downloaded Landsat image data for Dunedin, New Zealand from the USGS portal and loaded it in SNAP, as an RGB Image.  I crated small polygons for different types of surfaces like forest, barren earth, water and urban areas, and then opened this in R to examine the reflectance of these land use types for different bands.\n\n\n\nSpectral Reflectance\n\n\nThe figures above show, e.g. that band 5 has the highest reflectance for all of the selected land types, except water. In band 6 the pixels of the urban area had the highest reflectance. The second plot also shows how these classes vary, and we can see that water is most consistent, while urban and bare earth seem to have the most varying values of pixels. These values, of course depend on the polygons that were selected.\nHaving a practical example of a satellite image, was very helpful to realise what this data looks like and how it can be analysed. It was certainly an important realisation that working with this single, relatively small image requires a lot of memory for processing, which made me appreciate more the abilities of cloud computing for remote sensing data, especially when working on a larger scale, and with higher spatial and spectral resolutions data.\nWhen thinking about the vastness of available satellite imagery data, it’s easy to get overwhelmed. Before beginning to use such data for research questions, I definitely feel that it’s good to know how these sensors collect the data, why there are different bands, what they represent etc. As working with such data locally is naturally, rather difficult due to their size and computational expense needed to process them, I am curious to learn more about the cloud tools like GEE.\n\n\n\n\nLaw, W Boone, Peter Hiscock, Bertram Ostendorf, and Megan Lewis. 2021. “Using Satellite Imagery to Evaluate Precontact Aboriginal Foraging Habitats in the Australian Western Desert.” Scientific Reports 11 (1): 10755.\n\n\nLi, Wei, Jean-Daniel M Saphores, and Thomas W Gillespie. 2015. “A Comparison of the Economic Benefits of Urban Green Spaces Estimated with NDVI and with High-Resolution Land Cover Data.” Landscape and Urban Planning 133: 105–17.\n\n\nTaherzadeh, Ebrahim, and Helmi ZM Shafri. 2011. “Using Hyperspectral Remote Sensing Data in Urban Mapping over Kuala Lumpur.” In 2011 Joint Urban Remote Sensing Event, 405–8. IEEE."
  },
  {
    "objectID": "wk3.html#applications",
    "href": "wk3.html#applications",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.2 Applications",
    "text": "3.2 Applications\nThe applications of the methods topics covered this week are also numerous. As discussed, there are several types of corrections that are necessary, and many different ways to apply them. For example, one study evaluated different models of atmospheric corrections on raw Landsat data, in order to find the most appropriate ones for the coastal environment around Hong Kong (Nazeer, Nichol, and Yung 2014). The authors found that different models for correction perform best over different surfaces. They also point to the fact that local characteristics of the atmosphere around Hong Kong have an impact on how the correction model performs, as it was created for continental landmass areas, this shows the importance of using a correction model appropriate for the local area.\nTexture-based methods applied to images have largely been used to identify or classify certain features of the environment. One such example is a study by (Kit, Lüdeke, and Reckien 2012), that identified slums in Hyderabad using texture-based methods on satellite imagery. The detection relies on a line detection based on lacunarity calculation, which is a measure that uses a variance filter. The performance of this method is compared to a PCA calculation from multiple bands, and the line detection method is found to be a significantly better predictor of slums in this city.\nThis methodology of feature engineering with filters and texture-based methods can often be used as input to machine learning models in order to train a model for classifying features. For example, (Tuia et al. 2009) have used SVMs for classifying high resolution urban images. The authors point to the fact that such methods, though, are limited as they do not consider the spatial relation between pixel values. It was shown that by including the spatial information, the model’s classification accuracy improved significantly."
  },
  {
    "objectID": "wk3.html#glossary-of-terms",
    "href": "wk3.html#glossary-of-terms",
    "title": "3  Week 3",
    "section": "3.4 Glossary of terms",
    "text": "3.4 Glossary of terms\nUsing collection 2, surface reflactance level 2, tier 1."
  },
  {
    "objectID": "wk3.html#sensor-types",
    "href": "wk3.html#sensor-types",
    "title": "3  Week 3",
    "section": "3.1 Sensor types",
    "text": "3.1 Sensor types\nFirstly, a bit about the history of sensors - the first digital sensor put on a Landsat satellite was the Multispectral Scanner Camera (MSS) built by Virginia Norwood. Its innovative design that used a whisk broom, rather than a push broom, was initially reluctantly implemented by NASA, as previous RBV sensors, were limited to only a few bands. The data captured by the satellites was made publicly available in 2008.\n—Pivoting mirror not scanner."
  },
  {
    "objectID": "wk3.html#reflections",
    "href": "wk3.html#reflections",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections\nThe more technically-heavy material of this week has given me a good grounding in understanding some of the aspects of satellite data that we don’t directly deal with, as data comes analysis-ready. What I find particularly interesting are the kernel-based operations that can be applied to satellite data and what features they might show. I had not thought about applying kernel-based operations to images as filters and using such outputs as possible features that could show us something useful.\nI had a go at applying some filters on the 4th band of the image of Dunedin from previous weeks. The images below are the outputs of applying a 7x7 window and calculating the mean, variance, entropy etc.\n\n\n\nTexture measures of Dunedin\n\n\nSome of the measures may be difficult to interpret, as this was run on the band 4 representing infra-red - e.g. water is only distinctively shown in the mean texture, while high entropy, for example, shows up along the coastline. I then applied the PCA algorithm on the original bands of the data, which gave the principal components visible below. \nOverall, I found the material this week particularly interesting, especially about filters and kernel based operations done on the images.\n\n\n\n\nKit, Oleksandr, Matthias Lüdeke, and Diana Reckien. 2012. “Texture-Based Identification of Urban Slums in Hyderabad, India Using Remote Sensing Data.” Applied Geography 32 (2): 660–67.\n\n\nNazeer, Majid, Janet E Nichol, and Ying-Kit Yung. 2014. “Evaluation of Atmospheric Correction Models and Landsat Surface Reflectance Product in an Urban Coastal Environment.” International Journal of Remote Sensing 35 (16): 6271–91.\n\n\nTuia, Devis, Frédéric Ratle, Alexei Pozdnoukhov, and Gustavo Camps-Valls. 2009. “Multisource Composite Kernels for Urban-Image Classification.” IEEE Geoscience and Remote Sensing Letters 7 (1): 88–92."
  },
  {
    "objectID": "wk4.html#policy-discussions",
    "href": "wk4.html#policy-discussions",
    "title": "4  Week 4",
    "section": "4.1 Policy discussions",
    "text": "4.1 Policy discussions"
  },
  {
    "objectID": "wk4.html#policy-applicationns",
    "href": "wk4.html#policy-applicationns",
    "title": "4  Week 4",
    "section": "4.1 Policy applicationns",
    "text": "4.1 Policy applicationns\nLand Use and land cover - difference - physicla propery and what people use it for."
  },
  {
    "objectID": "wk1.html#summary",
    "href": "wk1.html#summary",
    "title": "1  Week 1 - Introduction to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nIn the first week of the module we were introduced to remote sensing, focusing on what satellite imagery is and how it iss collected. The first classification of sensor types, is into passive and active sources, the former use the sun’s energy reflected off the ground, while the latter use their own energy source to send signal to Earth and record it (e.g. SAR). The sensors record the signal by capturing electromagnetic waves, which can have different frequencies, enabling sensors to collect information about things that are not visible to human eye, due to varying spectral signatures of different objects. The figure below shows this spectrum of wavelengths.\n\n\n\nElectromagnetic spectrum. Image source: SkyWatch.com\n\n\nWe learned there are four resolutions of images captured by satellites - spatial, spectral, radiometric and temporal. Individual bands can be merged to create indices, which show specific characteristics of the Earth’s surface that we want to measure. In a situation where bands are not in the same spatial resolutions, images need to be resampled into the same resolution, in order to be able to work with such data conjointly, either by downscaling or upscaling.\nSatellite data, e.g. from Sentinel and Landsat satellites can be freely accessed from the Copernicus Open Access Hub. Software tools like QGIS and SNAP (specifically for Sentinel data) can be used to browse and analyse this data, e.g. for combining different bands to produce desired indices. Examples of such indices include false colour composite, the tasseled cap or atmospheric penetration composite, which allow us to visualise features of the environment that are not observable to the human eye."
  },
  {
    "objectID": "wk3.html#summary",
    "href": "wk3.html#summary",
    "title": "3  Week 3 - Corrections and Data Enhancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nThe first digital sensor put on a Landsat satellite was the Multispectral Scanner Camera (MSS) built by Virginia Norwood. Its innovative design was initially reluctantly implemented by NASA - the previous RBV sensors were limited to only a few bands. The data captured by the satellites began to be made publicly available in 2008.\n\n3.1.1 Corrections\nRaw satellite imagery require certain corrections before being ready for use in analysis. Even though, images come to us analysis-ready in most cases, it’s good to understand what types of corrections need to be applied, and why.\nGeometric correction This is a correction that is needed for images taken at off-nadir angle, meaning they are not looking directly down. The correction is done by mapping the points on a pre-correction image to the same points in the ground truth image, which can be done with simple OLS regression. This correction is basically like georeferencing an image without coordinates.\nAtmospheric correction This correction is needed when the signal going though the atmosphere is distorted resulting in haze and scattering in the images. This shows up in images as the adjacency effect, where pixel values switch their position. Also worth mentioning is that the radiance can be disturbed in different ways, e.g. it can scatter in the atmosphere, or the light can get absorbed by the particles in the atmosphere, which is atmospheric attenuation.\nOrthorectification / topographic correction This is used to force pixels to be viewed at nadir angle. To correct for this, we can use a cosine correction, which requires information on the sensor geometry and an elevation model of the area imaged.\n\n\n3.1.2 Radiometric Calibration\nRaw pixel values of a satellite image need to be converted from digital numbers to represent reflectance. In raw images, a Digital Number (DN) pixels represent image brightness or radiance recorded so the intensity of the electromagnetic radiation per pixel. Thus radiance is the amount of light recorded by the sensor, and reflectance is the ratio of signal reflected from an object, to the amount recorded by the sensor. Radiometric calibration is this process of converting DN to spectral radiance, calculated using values that the sensor was calibrated for, before deployed on satellites.\nAs mentioned, in practice, most of the data we use comes already analysis-ready. For Landsat, algorithms like LEDPAS or L8SR apply the relevant corrections, which are the Level 2 products.\n\n\n3.1.3 Enhancements and data joining\nCertain manipulations can be done of the corrected images. Firstly, we may need to join datasets as images naturally come in tiles. The edges of such tiles will usually overlap, in which case we can blend them into each other by mosaicking, or feathering.\nFilters Applying filters to an image is like applying a convolution kernel to an image. Different filters can be used to enhance, extract or smooth certain features of the image.\nTexture Various operations can be done with filters, e.g. calculating mean, variance, entropy and other operations of images. The value of every pixel will then reflect, e.g. the variance of the kernel centered on that pixel.\nWe can also stack images on top of each other, e.g. from different dates, which is data fusion.\nPCA Just how PCA, a dimensionality reduction method can be used on tabular data, where the number of variables is reduced, so is it the same for satellite imagery. Linear combinations of multiple bands are combined in a way that maximises the variation in pixel values thus reducing the spectral resolution."
  },
  {
    "objectID": "wk4.html#options-from-measuring",
    "href": "wk4.html#options-from-measuring",
    "title": "4  Week 4 - Policy applications",
    "section": "4.1 Options from measuring",
    "text": "4.1 Options from measuring\n\n4.1.1 Backscatter - problem affecting this imager coz of\n\nCorner reflections in urban environments\nShadowing - building behind another not imaged\nSpeckle - grainy, from scattering on ground - “salt and pepp\n\n\n\n4.1.2 Amplitude dataset\nWind affects detection of water bodeis, due to the water surface not being even\n\n\n4.1.3 Phase difference data\ndetecting shifts, elevationn models, eqarthqaukes unavailable in gee"
  },
  {
    "objectID": "wk4.html#aim",
    "href": "wk4.html#aim",
    "title": "4  Week 4 - Policy applications",
    "section": "4.2 Aim",
    "text": "4.2 Aim\nInforming policies with research ### Key global policy documents - directions New urban Agnda SDGs Local policy documents City mastrplans Who’s setting the policy and responsible for policy implementation"
  },
  {
    "objectID": "wk4.html#applications",
    "href": "wk4.html#applications",
    "title": "4  Week 4 - Policy applications",
    "section": "4.2 Applications",
    "text": "4.2 Applications\nIn order to address this issue from both sides, policies would need to be developed that for one identify the most important areas of farmlands that would need to be protected and secondly identify inner-city areas where densification is most suitable. For example around the now under-construction Melbourne Suburban Rail Loop stations where densification is being prioritised. This is where remote sensing data could help.\nUsing satellite imagery we could identify the most valuable areas which should be protected. The identification of soil quality and the valuation of natural environments are applications where remote sensing can be used (Hazeu et al. 2014). Furthermore for finding areas of densification, remote sensing might not be an obvious choice, but could help in assessing the greenness of such areas or e.g. their soil permeability - all aspects that are important in order for the city to build dense urban spaces that are good for the environment."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "CASA0023",
    "section": "About me",
    "text": "About me\nTo be added"
  },
  {
    "objectID": "wk4.html#summary",
    "href": "wk4.html#summary",
    "title": "4  Week 4 - Policy applications",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nI recently read an article about an issue that the city of Melbourne is facing, which could potentially benefit from analysis of remote sensing data (Wilson 2024). The issue described in the article is about the land on the city’s fringe being converted from important farmland into low-rise suburban developments. The land surrounding the city has some of the richest and most productive soils in the state of Victoria, which is important for both food security and the local economy. The developments encroaching onto this land are also associated with increased fire risk, which is a very important issue in this region of Australia (Stanley and March 2020).\nThere is a lack of policies in Melbourne that would address this issue and constrain where developments are allowed to happen, given the potential of the land for agriculture. Of course, completely halting new developments in Melbourne is a really bad idea, due to the city’s very unaffordable housing market and projected population growth. Curbing the supply of new housing would put even more pressure on the already strained property market. That is why it is important the city allows for more urban intensification, and designates areas where the city should be growing, not just through sprawl but also urban desnification which is an increasingly discussed topic in the city, which is being addressed with new policies. Therefore, the need for more housing in Melbourne is a twofold problem - firstly keeping the existing farmlands, and secondly building more affordable housing in the city."
  },
  {
    "objectID": "wk4.html#reflections",
    "href": "wk4.html#reflections",
    "title": "4  Week 4 - Policy applications",
    "section": "4.3 Reflections",
    "text": "4.3 Reflections\nThe approach outlined above of using remotely sensed data for this issue, may not be the best choice for all aspects of the problem. Firstly, it would require using very high-resolution imagery. Secondly, for analysis of areas of the city where densification potential is highest, a natural choice would be to rely on vector data of spatial units, using sociodemographic data or transport and infrastructure data.\nThe Melbourne Plan 2017-2050 mentions in its implementation the importance of using spatial data for planning new developments and open spaces (Victoria State Government 2017). The plan even mentions in one section how the city will “protect the right to farm in key locations within green wedges and peri-urban areas”. However, the methodology or any directions of how this will be achieved is lacking."
  },
  {
    "objectID": "wk6.html#summary",
    "href": "wk6.html#summary",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nThe Google Earth Engine (GEE) is a tool that enables planetary scale analysis of remote sensing data. The tool enables easy access of satllite data from Landsat, Sentinel and many other products directly on the platform, using the Javascript-based code editor. The extension to Python is actively being developed, particularly with the geemap Python package. The data processing is done on Google’s servers, which combined with the readily accessible datasets makes GEE an immensely powerful research tool.\nAll the operations and objects are stored on the server, and interacting with them is done by writing specific code in JS that packaged into JSON is sent to the server to execute. Using Google’s servers allows for parallelisation of operations, which enables extremely large computational operations to complete in a reasonable time frame. That is why it is for example important to use map functions, rather than loops. GEE will also dynamically scale the resolution of satellite imagery, based on the request for optimal viewing."
  },
  {
    "objectID": "wk6.html#applications",
    "href": "wk6.html#applications",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "5.2 Applications",
    "text": "5.2 Applications\nUse of GEE in publications has skyrocketed since 2015, overtaking the number of newly published research papers where analysis was performed using Python, R or QGIS. The review of literature using GEE in methodology revealed several interesting facts, which show the potential of GEE for global analysis and research [Velastegui-Montoya et al. (2023)]. The authors found GEE was used in studies in disciplines ranging from earth and environmental science, social science, to engineering.\nWhile browsing the literature, several other interesting papers caught my interest.For example [Liu et al. (2018)] use GEE to identify global urbanisation between 1990 to 2010, showing that urban land has increased by around 43% in these years. The authors estimate it using a custom Normalized Urban Areas Composite Index from Landsat 5 data, constructed as a base from several established indices, including NDVI. What is very important, is that the authors make the dataset from this analysis publicly available. Another study used the MODIS satellite data for wildfire damage assessment for all of Australia, based on land cover classification of pre and post-fire images [Seydi et al. (2021)]."
  },
  {
    "objectID": "wk6.html#reflections",
    "href": "wk6.html#reflections",
    "title": "5  Week 5 - Google Earth Engine",
    "section": "5.3 Reflections",
    "text": "5.3 Reflections\nIn the practical sessions I got to use GEE for the first time, which involved loading some Landsat data and running certain operations. Playing around with the data in GEE, I certainly realised what potential it has for large-scale analysis. I added some satellite data for Melbourne and understood in practice how mosaicking is applied to multiple overlapping images, as well as applied some texture measures and PCA to the image.\n\n\n\nExploring Google Earth Engine for the first time! (of course in Melbourne)\n\n\nOverall, it seems to me that the power of GEE is greatly underestimated, as it enables truly impressive large-scale analysis to be run by anyone with internet access, for free. I previously did not realise how powerful this tool is and I cannot wait to see where this course will take me further in regards to GEE. As a sidenote, I am not a huge fan of the GUI of GEE - Google could probably do much better and I hope they soon will.\n\n\n\n\nLiu, Xiaoping, Guohua Hu, Yimin Chen, Xia Li, Xiaocong Xu, Shaoying Li, Fengsong Pei, and Shaojian Wang. 2018. “High-Resolution Multi-Temporal Mapping of Global Urban Land Using Landsat Images Based on the Google Earth Engine Platform.” Remote Sensing of Environment 209: 227–39.\n\n\nSeydi, Seyd Teymoor, Mehdi Akhoondzadeh, Meisam Amani, and Sahel Mahdavi. 2021. “Wildfire Damage Assessment over Australia Using Sentinel-2 Imagery and MODIS Land Cover Product Within the Google Earth Engine Cloud Platform.” Remote Sensing 13 (2): 220.\n\n\nVelastegui-Montoya, Andrés, Néstor Montalván-Burbano, Paúl Carrión-Mero, Hugo Rivera-Torres, Luı́s Sadeck, and Marcos Adami. 2023. “Google Earth Engine: A Global Analysis and Future Trends.” Remote Sensing 15 (14): 3675."
  },
  {
    "objectID": "wk7.html#summary",
    "href": "wk7.html#summary",
    "title": "6  Week 7 - Classification",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nThis week I learned more about applying machine learning methods to raster data. We looked specifically at two types of analysis - classification and regression, depending on the problem we need to solve. Such models utilise an expert system, which is a mechanism that draws conclusions from a knowledge base it was exposed to during training. Machine learning on raster images can be broadly divided into object-based and pixel-based analysis.\nDecision Trees\nClassification\nThis method is an algorithm for splitting up the dataset into classes based on values of predictors. Decision trees are built up of decision nodes at which the split of the datapoints occurs, and leaves which match the data that falls into that leaf to a particular class. Basically it’s classifying the data based on many if else statements. However, the mechanism for constructing such trees can be more complex.\nIn order to decide how best to split the data, several measures can be used, such that the decision nodes split the data in the most effective way, including entropy, information gain, or the most commonly used - Gini impurity, which is calculated for every decision node, and takes the form of the equation below:\n\\[\nginiImpurity(D) = 1 - \\sum_{i=1}^{k}{p_i^2}\n\\]\nwhere \\(p\\) is the probability of datapoints belonging to class \\(i\\) at node \\(D\\), and \\(k\\) is the number of classes. When building the classification tree, the nodes with lowest Gini impurity are selected. The trees are evaluated iteratively, fitting lines to all groups and keeping splits with lowest sum of squared residuals (SSR).\nThe problem of overfitting decision trees - when they learn too granular aspects of the data and cannot generalise to the unseen data can be combated in several ways. The most common method is pruning based on the weakest link. This means removing decision nodes, which are the least important in classifying training data into the correct classes. This is based on the tree score, which can be calculated using the function below:\n\\[\ntreeScore(Tree) = SSR + \\alpha T\n\\] where is the tree penalty parameter and \\(T\\) is the number of leaves. The process works by iteratively increment alpha until the tree score starts to decrease. Cross validation can be used to find the best alpha.\nRegression Trees\nThe same methodology of decision trees extends to regression problems, in which we need to predict a number rather than a class. The tree is built by splitting the dataset into groups along one of the axes, and a decision node is added at the point where the SSR resulting from that division is lowest. The first decision node is chose for the predictor whose split results in the lowest SSR, and the process is repeated iteratively until the number of splits reaches the limit imposed by the settings chosen, such as minimum number of data points for a leaf.\n\n\n\nIllustration of Reegression Tree applied to a dataset. Source: datacamp.com\n\n\n\n6.1.1 Random forest\nThis method generalises decision trees by building many random decision trees, which we have little control of how individual trees are built. However, the methodology builds many simple trees, which overall vote over what the prediction for a new datapoint should be. The training of random forests benefits from bootstrapping, which is a method that randomly subsets the training data, such that every tree is trained on different subset, which means different values will be fed to different trees. An out-of-bag hold out set of the data can be used to evaluate the trained model.\nSupport Vector Machines SVMs are a method that finds a hyperplane in the multidimensional feature space, which best divides that data into classes. The method can benefit from applying kernel functions, which project the data points onto hyperplanes, which are easier to classify. The hyperparameters, which control the model can be tuned, e.g. using grid search, which tests many predefined values of parameters (very expensive), or using an optimisation algorithm, such as simulated annealing, swarm optimisation, or genetic algorithms.\nUnsupervised methods Additionally, unsupervised methods can also be used for identifying unspecified classes. Algorithms such as DBSCAN or k-means clustering, will assign data points to optimal classes, based on the data. When a cluster is identified that comprises two known classes, cluster busting can be done, which is reapplying a clustering algorithm to that one cluster.\nApplying these methods to satellite imagery can be done for every pixel, where e.g. a land cover class is assigned to every pixel (classification) or an index value (regression), and features are channels."
  },
  {
    "objectID": "wk7.html#applications",
    "href": "wk7.html#applications",
    "title": "6  Week 7 - Classification",
    "section": "6.2 Applications",
    "text": "6.2 Applications\nThere are heaps of examples of research papers applying machine learning methods to urban research. The first one I wanted to focus on here is a paper that takes a two-pronged fuzzy approach to classification, in that land cover classes are assigned probabilities to pixels, rather than definitive classes (Shackelford and Davis 2003). Similar classes such as Roads and Buildings would in such a case be most difficult to classify, especially since pixel-based analysis doesn’t consider spatial dependence. After performing pixel-based classification, the authors develop a second method, that performs object-based segmentation of the image based on similarity of classes in nearby pixels, which makes the output more consistent, resulting in an overall more flexible methodology.\nAnother paper by (Goldblatt et al. 2016) is aimed at identifying boundaries of urban areas using a pixel based analysis of remotely sensed data, using SVM and Random Forest models. Because the authors develop the model in GEE, the research can be easily replicated for other countries, which poses a great opportunity for replicating the methodology."
  },
  {
    "objectID": "wk7.html#reflections",
    "href": "wk7.html#reflections",
    "title": "6  Week 7 - Classification",
    "section": "6.3 Reflections",
    "text": "6.3 Reflections\nThis week was actually quite unique as it offered a perspective on working with image data I had not considered before. Prior to this lecture, I had only learned about object-based analysis of images with convolutional operations, and specifically using neural networks to extract and encode information from images in order to classify or identify features from images. This lecture opened my eyes to much simpler way of working with satellite imagery that uses only pixel values of various bands (features) to predict such outcomes.\nThis enables the use of classical machine learning algorithms like e.g. Random Forests in the context of raster data, by treating is just like tabular data. This gets rid of the spatial dependence of pixels that are next to each other, which of course is absolutely crucial, but such methods are significantly computationally cheaper than convolutional neural networks.\n\n\n\n\nGoldblatt, Ran, Wei You, Gordon Hanson, and Amit K Khandelwal. 2016. “Detecting the Boundaries of Urban Areas in India: A Dataset for Pixel-Based Image Classification in Google Earth Engine.” Remote Sensing 8 (8): 634.\n\n\nShackelford, Aaron K, and Curt H Davis. 2003. “A Combined Fuzzy Pixel-Based and Object-Based Approach for Classification of High-Resolution Multispectral Data over Urban Areas.” IEEE Transactions on GeoScience and Remote Sensing 41 (10): 2354–63."
  },
  {
    "objectID": "wk4.html#references",
    "href": "wk4.html#references",
    "title": "4  Week 4 - Policy applications",
    "section": "4.4 References",
    "text": "4.4 References\n\n\n\n\nHazeu, Gerard, Pavel Milenov, Bas Pedroli, Vessela Samoungi, Michiel Van Eupen, and Vassil Vassilev. 2014. “High Nature Value Farmland Identification from Satellite Imagery, a Comparison of Two Methodological Approaches.” International Journal of Applied Earth Observation and Geoinformation 30: 98–112.\n\n\nStanley, Janet, and Alan March. 2020. “Black Saturday: Urban Sprawl and Climate Change Remain Key Dangers.” Pursuit. https://pursuit.unimelb.edu.au/articles/black-saturday-urban-sprawl-and-climate-change-remain-key-dangers.\n\n\nVictoria State Government. 2017. “Plan Melbourne 2017–2050.” 2017. https://www.planning.vic.gov.au/guides-and-resources/strategies-and-initiatives/plan-melbourne.\n\n\nWilson, Katherine. 2024. “Guardian News & Media Produces a Variety of Content with Funding from Outside Parties.” The Guardian, May. https://www.theguardian.com/info/2017/may/22/guardian-news-media-funding-and-investment."
  },
  {
    "objectID": "wk9.html#summary",
    "href": "wk9.html#summary",
    "title": "8  Week 9 - Synthetic Aperture Radar",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nSynthetic Aperture Radar (SAR) is an active type of sensor that sends a signal at a specific frequency to Earth’s surface and records the signal in that same frequency coming back. The choice of the frequency of course will determine what the sensor is able to identify. For example sea band radio waves (4-8 GHz) can be used for detecting ice or ocean maritime activity. This is why depending on the wavelength we are able to penetrate e.g. through the tree canopy, as well as clouds. Another advantage is that this can collect data at night, as sun’s energy is not utilised.\nAperture of the radar is of significance, as that determines the resolution of the collected data. This is controlled by the size of the antenna, and the longer the antenna the higher resolution we could collect. For that reason, this sensor utilises the movement of the satellite in orbit to “expand” the antenna, because it is continuously sending signal to Earth imaging the specific location from different points in orbit. Hence the name - synthetic aperture radar. The image below illustrates how this works.\n\n\n\nIllustration of SAR. Source: DLR (CC BY-NC-ND 3.0)\n\n\nAnother notable aspect of SAR imagery is the polarisation of the signal. Different sensors will have different polarisation (either vertical or horizontal), which affects what materials are recorded. For example buildings and trees are most sensitive to sensor sending and receiving horizontal polarisations, while bare earth to vertical-vertical (VV). Therefore, for example it is of significance to capturing water’s surface, whether the imagery is collected on a windy or calm day, as rough water surface will reflect signal differently.\nBackscatter or amplitude of the signal, as well as its phase are also of significance. Phase is when in the wave cycle the signal reaches the sensor (whether its at a peak or trough of the wave). This is the technique, which enables change detection with very high accuracy. This is calle Interferometric SAR (InSAR), which can be used for creating Digital Elevation Models (DEMs). Furthermore, Differential InSAR (DInSAR) can also be used for detecting change after earthquakes, landslides or other events that had an impact on the earth’s surface, but also buildings after for example bombings.\nSAR imagery when catpured, of course also comes with lots of metadata, which is crucial to understand and analyse the imagery, for example it will matter whether the data was recorded on the south-to-north or north-to-south orbit pass. Moreover, SAR data will often be on a power scale, which necessitates the use of appropriate transformation methods. In practice, perhaps the most influential usecase of SAR data is for change detection, which is based on a ratio of the image after a change event to the image prior. There have been many ways developed for quantifying change, depending on the intended use case, such as inverted ratio which works in ways that gives changed pixels more importance."
  },
  {
    "objectID": "wk9.html#reflections",
    "href": "wk9.html#reflections",
    "title": "8  Week 9 - Synthetic Aperture Radar",
    "section": "9.1 Reflections",
    "text": "9.1 Reflections\nTODO"
  },
  {
    "objectID": "wk8.html",
    "href": "wk8.html",
    "title": "7  Week 8 - Assessment of classification models",
    "section": "",
    "text": "8 Applications\nTODO"
  },
  {
    "objectID": "wk8.html#summary",
    "href": "wk8.html#summary",
    "title": "7  Week 8 - Assessment of classification models",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nThis week the material continued on classification, focusing on aspects like object-based analysis and how we can assess classification results. Firstly, let’s look at object-based analysis, as opposed to pixel-based."
  },
  {
    "objectID": "wk8.html#reflections",
    "href": "wk8.html#reflections",
    "title": "7  Week 8 - Assessment of classification models",
    "section": "8.1 Reflections",
    "text": "8.1 Reflections\nTODO"
  },
  {
    "objectID": "wk9.html",
    "href": "wk9.html",
    "title": "8  Week 9 - Synthetic Aperture Radar",
    "section": "",
    "text": "9 Applications\nTODO"
  }
]