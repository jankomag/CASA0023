# Week 7 - Classification

## Summary

This week I learned more about applying machine learning methods to raster data. We looked specifically at two types of analysis - classification and regression, depending on the problem we need to solve. Such models utilise an expert system, which is a mechanism that draws conclusions from a knowledge base it was exposed to during training. Machine learning on raster images can be broadly divided into object-based and pixel-based analysis.

**Decision Trees**

***Classification***

This method is an algorithm for splitting up the dataset into classes based on values of predictors. Decision trees are built up of decision nodes at which the split of the datapoints occurs, and leaves which match the data that falls into that leaf to a particular class. Basically it's classifying the data based on many if else statements. However, the mechanism for constructing such trees can be more complex.

In order to decide how best to split the data, several measures can be used, such that the decision nodes split the data in the most effective way, including entropy, information gain, or the most commonly used - Gini impurity, which is calculated for every decision node, and takes the form of the equation below:

$$
giniImpurity(D) = 1 - \sum_{i=1}^{k}{p_i^2}
$$

where $p$ is the probability of datapoints belonging to class $i$ at node $D$, and $k$ is the number of classes. When building the classification tree, the nodes with lowest Gini impurity are selected. The trees are evaluated iteratively, fitting lines to all groups and keeping splits with lowest sum of squared residuals (SSR).

The problem of overfitting decision trees - when they learn too granular aspects of the data and cannot generalise to the unseen data can be combated in several ways. The most common method is pruning based on the weakest link. This means removing decision nodes, which are the least important in classifying training data into the correct classes. This is based on the tree score, which can be calculated using the function below:

$$
treeScore(Tree) = SSR + \alpha T
$$ where \alpha is the tree penalty parameter and $T$ is the number of leaves. The process works by iteratively increment alpha until the tree score starts to decrease. Cross validation can be used to find the best alpha.

Pruning is used to build the tree, aiming to optimise the number of decisions in order to make the trees parsimonious.

**Regression Trees**

The same methodology of decision trees extends to regression problems, in which we need to predict a number rather than a class. The tree is built by splitting the dataset into groups along one of the axes, and a decision node is added at the point where the SSR resulting from that division is lowest. The first decision node is chose for the predictor whose split results in the lowest SSR, and the process is repeated iteratively until the number of splits reaches the limit imposed by the settings chosen, such as minimum number of data points for a leaf.

![Illustration of Reegression Tree applied to a dataset. [Source: datacamp.com](https://www.datacamp.com/tutorial/decision-trees-R)](res/regtree.png)

### Random forest

This method generalises decision trees by building many random decision trees, which we have little control of how individual trees are built. However, the methodology builds many simple trees, which overall vote over what the prediction for a new datapoint should be. The training of random forests benefits from bootstrapping, which is a method that randomly subsets the training data, such that every tree is trained on different subset, which means different values will be fed to different trees. An out-of-bag hold out set of the data can be used to evaluate the trained model.

**Support Vector Machines** SVMs are a method that finds a hyperplane in the multidimensional feature space, which best divides that data into classes. The method can benefit from applying kernel functions, which project the data points onto hyperplanes, which are easier to classify. The hyperparameters, which control the model can be tuned, e.g. using grid search, which tests many predefined values of parameters (very expensive), or using an optimisation algorithm, such as simulated annealing, swarm optimisation, or genetic algorithms.

**Unsupervised methods** Additionally, unsupervised methods can also be used for identifying unspecified classes. Algorithms such as DBSCAN or k-means clustering, will assign data points to optimal classes, based on the data. When a cluster is identified that comprises two known classes, cluster busting can be done, which is reapplying a clustering algorithm to that one cluster.

Applying these methods to satellite imagery can be done for every pixel, where e.g. a land cover class is assigned to every pixel (classification) or an index value (regression), and features are channels.

## Applications

There are heaps of examples of research papers applying machine learning methods to urban research. The first one I wanted to focus on here is a paper that takes a two-pronged fuzzy approach to classification, in that land cover classes are assigned probabilities to pixels, rather than definitive classes [@shackelford2003combined]. Similar classes such as Roads and Buildings would in such a case be most difficult to classify, especially since pixel-based analysis doesn't consider spatial dependence. After performing pixel-based classification, the authors develop a second method, that performs object-based segmentation of the image based on similarity of classes in nearby pixels, which makes the output more consistent, resulting in an overall more flexible methodology.

Another paper by [@goldblatt2016detecting] is aimed at identifying boundaries of urban areas using a pixel based analysis of remotely sensed data, using SVM and Random Forest models. Because the authors develop the model in GEE, the research can be easily replicated for other countries, which poses a great opportunity for replicating the methodology.

## Reflections

This week was actually quite unique as it offered a perspective on working with image data I had not considered before. Prior to this lecture, I had only learned about object-based analysis of images with convolutional operations, and specifically using neural networks to extract and encode information from images in order to classify or identify features from images. This lecture opened my eyes to much simpler way of working with satellite imagery that uses only pixel values of various bands (features) to predict such outcomes.

This enables the use of classical machine learning algorithms like e.g. Random Forests in the context of raster data, by treating is just like tabular data. This gets rid of the spatial dependence of pixels that are next to each other, which of course is absolutely crucial, but such methods are significantly computationally cheaper than convolutional neural networks.
