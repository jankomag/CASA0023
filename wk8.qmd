# Week 8 - Assessment of classification models

## Summary

This week the material continued on classification, focusing on aspects like object-based analysis and how we can assess classification results. Firstly, let's look at object-based, as opposed to pixel-based analysis.

Object-based analysis utilises supercells, which are created by joining pixels into objects, according to the similarity of nearby pixels, aiming to maximise the homogeneity of pixel values within superpixels. This for example helps with the issue of spatial autocorrelation between training and test data. To create supercells an algorithm such as Simple Linear Iterative Clustering can be used, which iteratively joins pixels into objects based on similarity.

![Supercells. [Source: Jakub Nowosad](https://jakubnowosad.com/ogh2021/#5)](res/supercell.png)

Another type of analysis uses the opposite approach - sub-pixel analysis. This is relevant when a pixel could be considerd as having several classes, e.g. lying at the intersection of more than one class. In such cases we can use Spectral Mixture Analysis, which estimates the proportion of different classes per pixel. This is related to probabilistic classification, where a pixel may be assigned a probability of belonging to the given class.

**Accuracy Assessment**

Classification accuracy assessment is measured with several metrics. Firstly, there are the Producer accuracy (recall), which is calculated as the true positive $(TP)$ rate over $(TP + FP)$, and the Userâ€™s accuracy (precision), measuring the rate of pixels being misclassified per specific class - $(TP)/(TP+FP)$. The overall accuracy can be calculated from these.

Another measure of accuracy of classification is the kappa coefficient, which however is regarded as not a very appropriate metric. It is defined as

The confusion matrix will often be created for a classification problem, which shows for which classes the model predicts correctly and not. F1 score doesnt account for True negative Use ROC and AUC to compare models

Another issue is when the data are not balanced in regards to classes,in which case we may use the F1 score, which aims to account for the imbalance and considers relative performance.

$$F1 = \frac{2 \times \text{TP}}{2 \times \text{TP} + \text{FP} + \text{FN}}$$
We also have the Receiver Operating Characteristic Curve, which uses the true positive rate and false positive rate to show how the predictions are made. Area under the curver give a quantifiable measure fo this method.

Another importand consideration is the train-test split, which is a fundamental step before training any modelm in order to avoid training the model too much on the traning data, and evaluating it on unseen. An even more appropriate approach is cross validation, which trains multiple times on different partitions of the dataset into the training and evaluates it on the testing. To account for spatial autocrrelation of testing and training data we need to be usiug spatial cross vlaidation, which ensures that points from different locations are used for traingna nad testin, limiting the risk of autocorrelation in testing an dtraining data. Additionally, spatial partitioning can be done to distinguish between the objects.

# Applications

Paper 1 assesses the pervformance of 6 different classification algorithms for land-use/land-cover using different assessment criteria. The assessment criteria used were RoC, kappa coefficient and  

## Reflections

It seems to me that for urban problems, using object-based analysis such as convolutional neural networks is more important when working with very high resolution imagery (sub-meter resolution), while it may be less important when we have low-resolution imagery like 10 meters, in which case pixel-based analysis may be more efficient. That's because the features, which would really make a difference for identifying in an urban environment would likely be quite granular, and that's when spatial aspect of images would be most important to consider. But of course, this depends on the particular problem.


so many assessment accuracy clasfficqaiton, oh what to choose?
